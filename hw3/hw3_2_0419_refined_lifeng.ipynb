{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "yoj3v7b5Nciw"
   },
   "source": [
    "## Problem2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 289
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 4224,
     "status": "ok",
     "timestamp": 1555732116532,
     "user": {
      "displayName": "Lifeng W",
      "photoUrl": "",
      "userId": "04920199623700226835"
     },
     "user_tz": 240
    },
    "id": "IvGFIzH4_wnQ",
    "outputId": "f55fd1bf-b468-46f2-a240-9a91ecd04b00",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive/\n",
      "total 114865\n",
      "-rw------- 1 root root 15680000 Apr 13 15:03 binarized_mnist_test.amat\n",
      "-rw------- 1 root root 78400000 Apr 13 15:12 binarized_mnist_train.amat\n",
      "-rw------- 1 root root 15680000 Apr 13 15:14 binarized_mnist_valid.amat\n",
      "-rw------- 1 root root    29106 Apr 20 03:45 hw3_2_0419_from_qiang.ipynb\n",
      "-rw------- 1 root root    35317 Apr 19 17:55 hw3_2_2_merged_0418.ipynb\n",
      "-rw------- 1 root root    61366 Apr 20 02:52 hw3_3.ipynb\n",
      "-rw------- 1 root root  3758307 Apr 15 00:57 model_20190415-005730.pkl\n",
      "-rw------- 1 root root  3758648 Apr 20 03:27 params.pkl\n",
      "drwx------ 2 root root     4096 Apr 15 00:48 reconstructed\n",
      "-rw------- 1 root root     2677 Apr 14 18:52 VAE_Final_Try.ipynb\n",
      "-rw------- 1 root root    13643 Apr 14 03:31 VAE.ipynb\n",
      "-rw------- 1 root root   143969 Apr 14 03:59 VAE-xiao.ipynb\n",
      "-rw------- 1 root root    16657 Apr 18 21:08 VAE-xiao+VALID.ipynb\n",
      "-rw------- 1 root root    35661 Apr 17 18:05 VAE-ying_0416.ipynb\n"
     ]
    }
   ],
   "source": [
    "from os.path import exists\n",
    "from google.colab import drive\n",
    "import os\n",
    "\n",
    "drive.mount('/content/drive/', force_remount = True)\n",
    "# change to your own directory\n",
    "os.chdir(\"drive/My Drive/Colab_20190413\")\n",
    "!ls -al"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 4630,
     "status": "ok",
     "timestamp": 1555732116963,
     "user": {
      "displayName": "Lifeng W",
      "photoUrl": "",
      "userId": "04920199623700226835"
     },
     "user_tz": 240
    },
    "id": "_DUFq_udfdxf",
    "outputId": "7fd92721-f9c1-4300-ab16-d559ccebdae8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "import random\n",
    "import math\n",
    "from torch.utils.data import DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn.functional as F\n",
    "\n",
    "cuda = True if torch.cuda.is_available() else False\n",
    "Tensor = torch.cuda.FloatTensor if cuda else torch.FloatTensor\n",
    "print(cuda)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "VnmFLcb20-Hm"
   },
   "source": [
    "**Load data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wul9zFkKySAd"
   },
   "outputs": [],
   "source": [
    "data_dir = \"./\"    # Change here if necessary\n",
    "\n",
    "train_data = np.loadtxt(data_dir + \"binarized_mnist_train.amat\")\n",
    "valid_data = np.loadtxt(data_dir + \"binarized_mnist_valid.amat\")\n",
    "test_data = np.loadtxt(data_dir + \"binarized_mnist_test.amat\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "L3wsF3FS1Dgq"
   },
   "source": [
    "**Model definition**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "K-Cvo-5YrTOI"
   },
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, z_dim = 100):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.z_dim = z_dim\n",
    "        self.encode = nn.Sequential(\n",
    "            nn.Conv2d(1, 32, kernel_size = (3,3)),     # 28 -> 26\n",
    "            nn.ELU(),\n",
    "            nn.AvgPool2d(kernel_size = 2, stride = 2), # 26 -> 13\n",
    "            nn.Conv2d(32, 64, kernel_size = (3,3)),    # 13 -> 11\n",
    "            nn.ELU(),\n",
    "            nn.AvgPool2d(kernel_size = 2, stride = 2), # 11 -> 5\n",
    "            nn.Conv2d(64, 256, kernel_size = (5,5)),   # 5  -> 1\n",
    "            nn.ELU(),\n",
    "        )\n",
    "        self.linear1 = nn.Linear(in_features = 256, out_features = 2*z_dim)\n",
    "        \n",
    "    def forward(self, x):                          # [batch_size, 1, 28, 28]\n",
    "        q = self.encode(x)                         # [batch_size, 256, 1, 1]\n",
    "        q = self.linear1(q.view(q.size(0), -1))    # [batch_size, 200]\n",
    "        mu, log_var = q[:,:z_dim], q[:, z_dim: ]   # [batch_size, 100] \n",
    "        return mu, log_var\n",
    "    \n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, z_dim = 100):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.z_dim = z_dim\n",
    "        self.linear2 = nn.Linear(in_features = z_dim, out_features = 256)\n",
    "        self.decode = nn.Sequential(\n",
    "            nn.ELU(),\n",
    "            nn.Conv2d(256, 64, kernel_size=5, padding=4), \n",
    "            nn.ELU(),\n",
    "            nn.UpsamplingBilinear2d(scale_factor=2),#mode='bilinear'\n",
    "            nn.Conv2d(64, 32, kernel_size=3, padding=2),\n",
    "            nn.ELU(),\n",
    "            nn.UpsamplingBilinear2d(scale_factor=2),#mode='bilinear'\n",
    "            nn.Conv2d(32, 16, kernel_size=3, padding=2),\n",
    "            nn.ELU(),\n",
    "            nn.Conv2d(16, 1, kernel_size=3, padding=2)\n",
    "        )\n",
    "        \n",
    "    def forward(self, z):\n",
    "        x_ = self.linear2(z).view(z.size(0),-1, 1,1) # [batch_size, 256, 1, 1]\n",
    "        x_ = self.decode(x_)                       #[batch_size, 1, 28, 28] \n",
    "        return x_\n",
    "\n",
    "class VAE(nn.Module):\n",
    "    def __init__(self, z_dim = 100):\n",
    "        super(VAE, self).__init__()\n",
    "        self.encoder = Encoder(z_dim = z_dim)\n",
    "        self.decoder = Decoder(z_dim = z_dim)\n",
    "        \n",
    "    def reparameterize(self, mu, log_var):\n",
    "        std = torch.exp(0.5 * log_var)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + torch.mul(eps, std)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        mu, log_var = self.encoder(x)\n",
    "        z = self.reparameterize(mu, log_var)\n",
    "        x_ = self.decoder(z)\n",
    "        return x_, mu, log_var\n",
    "    \n",
    "def compute_elbo(recon_x, x, mu, log_var):    \n",
    "    # compute log p_{\\theta} (x)\n",
    "    recon_x = recon_x.view(recon_x.size(0), -1)\n",
    "    x = x.view(x.size(0), -1)\n",
    "    \n",
    "    # convert to the probabilities of each pixel to be 1, [batch_size, 1]\n",
    "    output_px = torch.sigmoid(recon_x) \n",
    "    \n",
    "    # compute the log probabilities of \n",
    "    # each reconstructed pixel value = original binary value\n",
    "    # this is the binary cross entropy\n",
    "    log_px = torch.log(output_px * x + (1- output_px) * (1-x))\n",
    "    \n",
    "    # log prob for a reconstructed image is original image x, [batch_size, 1]\n",
    "    log_px = torch.sum(log_px, dim = 1, keepdim = True)\n",
    "    log_px = torch.mean(log_px) # mean over batch_size samples\n",
    "    # BCE = -1 * log_px         # relationship between BCE and lop_px\n",
    "    \n",
    "    # [batch_size, 1]\n",
    "    KLD = 0.5 * torch.sum((-1 - log_var + torch.pow(mu, 2) + torch.exp(log_var)), \n",
    "                          dim = 1, keepdim = True)\n",
    "    KLD = torch.mean(KLD) # mean over batch_size samples\n",
    "    # mean elbo over batch_size samples, zero dimension\n",
    "    elbo = log_px - KLD\n",
    "    return elbo, -log_px, KLD\n",
    "    \n",
    "def compute_loss(recon_x, x, mu, log_var):\n",
    "    batch_size = recon_x.size(0) # \n",
    "    # keep data and dimension, [batch_size, 1, 28, 28]\n",
    "    BCE = F.binary_cross_entropy_with_logits(recon_x, x, reduction=\"none\") \n",
    "    # sum all losses(of pixels) in one image, [batch_size, 1]\n",
    "    BCE = torch.sum(BCE.view(batch_size, -1), dim = 1, keepdim = True)\n",
    "    # compute average BCE over batch_size, zero dimension, []\n",
    "    BCE = torch.mean(BCE) \n",
    "    # [batch_size, 1]\n",
    "    KLD = 0.5 * torch.sum((-1 - log_var + torch.pow(mu, 2) + torch.exp(log_var)), \n",
    "                          dim = 1, keepdim = True)\n",
    "    # average over batch_size, zero dimensiona, []\n",
    "    KLD = torch.mean(KLD)\n",
    "    return BCE + KLD, BCE, KLD\n",
    "\n",
    "def data_loader(data, batch_size = 64):\n",
    "    \"\"\"\n",
    "    params\n",
    "        data: np.array, [sample_size, 784]\n",
    "    \"\"\"\n",
    "    image_data = torch.unsqueeze(Tensor(data.reshape(-1, 28, 28)), 1)\n",
    "    return DataLoader(image_data, batch_size = batch_size, shuffle = True)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "HtjEaT681Nco"
   },
   "source": [
    "**Training**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 3111
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 257139,
     "status": "ok",
     "timestamp": 1555732369513,
     "user": {
      "displayName": "Lifeng W",
      "photoUrl": "",
      "userId": "04920199623700226835"
     },
     "user_tz": 240
    },
    "id": "BtCWjdpm7n_v",
    "outputId": "6bc92ad7-c427-4bf4-f75b-f74e7388d4fb"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/upsampling.py:129: UserWarning: nn.UpsamplingBilinear2d is deprecated. Use nn.functional.interpolate instead.\n",
      "  warnings.warn(\"nn.{} is deprecated. Use nn.functional.interpolate instead.\".format(self.name))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch[1/20], Step [  100], loss: 213.6074 BCE 203.2597, KLD: 10.3477\n",
      "Epoch[1/20], Step [  200], loss: 173.4395 BCE 158.4358, KLD: 15.0037\n",
      "Epoch[1/20], Step [  300], loss: 175.4763 BCE 157.2164, KLD: 18.2599\n",
      "Epoch[1/20], Step [  400], loss: 158.8159 BCE 138.2292, KLD: 20.5866\n",
      "Epoch[1/20], Step [  500], loss: 142.7286 BCE 122.5219, KLD: 20.2068\n",
      "Epoch[1/20], Step [  600], loss: 143.6062 BCE 122.1241, KLD: 21.4821\n",
      "Epoch[1/20], Step [  700], loss: 138.2079 BCE 114.8305, KLD: 23.3774\n",
      "== Epoch[1/20]: elbo on valid set: -136.1425 ==\n",
      "\n",
      "Epoch[2/20], Step [  100], loss: 137.2505 BCE 114.2415, KLD: 23.0090\n",
      "Epoch[2/20], Step [  200], loss: 130.9104 BCE 107.5478, KLD: 23.3626\n",
      "Epoch[2/20], Step [  300], loss: 126.2803 BCE 102.8191, KLD: 23.4612\n",
      "Epoch[2/20], Step [  400], loss: 125.6329 BCE 101.7326, KLD: 23.9004\n",
      "Epoch[2/20], Step [  500], loss: 113.6006 BCE 90.2551, KLD: 23.3454\n",
      "Epoch[2/20], Step [  600], loss: 119.7551 BCE 95.4663, KLD: 24.2888\n",
      "Epoch[2/20], Step [  700], loss: 115.6446 BCE 92.5809, KLD: 23.0637\n",
      "== Epoch[2/20]: elbo on valid set: -116.3842 ==\n",
      "\n",
      "Epoch[3/20], Step [  100], loss: 115.1023 BCE 90.5047, KLD: 24.5975\n",
      "Epoch[3/20], Step [  200], loss: 117.9079 BCE 93.6433, KLD: 24.2646\n",
      "Epoch[3/20], Step [  300], loss: 114.4405 BCE 90.0563, KLD: 24.3842\n",
      "Epoch[3/20], Step [  400], loss: 116.2692 BCE 91.3484, KLD: 24.9208\n",
      "Epoch[3/20], Step [  500], loss: 109.7238 BCE 84.8728, KLD: 24.8509\n",
      "Epoch[3/20], Step [  600], loss: 111.0502 BCE 86.4390, KLD: 24.6112\n",
      "Epoch[3/20], Step [  700], loss: 105.4455 BCE 80.7616, KLD: 24.6840\n",
      "== Epoch[3/20]: elbo on valid set: -109.1387 ==\n",
      "\n",
      "Epoch[4/20], Step [  100], loss: 111.4934 BCE 85.3560, KLD: 26.1374\n",
      "Epoch[4/20], Step [  200], loss: 108.8939 BCE 83.4539, KLD: 25.4400\n",
      "Epoch[4/20], Step [  300], loss: 110.2491 BCE 84.6190, KLD: 25.6301\n",
      "Epoch[4/20], Step [  400], loss: 103.8501 BCE 78.8334, KLD: 25.0167\n",
      "Epoch[4/20], Step [  500], loss: 102.5931 BCE 77.9471, KLD: 24.6460\n",
      "Epoch[4/20], Step [  600], loss: 97.5101 BCE 73.2280, KLD: 24.2821\n",
      "Epoch[4/20], Step [  700], loss: 103.9941 BCE 78.6530, KLD: 25.3411\n",
      "== Epoch[4/20]: elbo on valid set: -105.4878 ==\n",
      "\n",
      "Epoch[5/20], Step [  100], loss: 106.4983 BCE 80.8800, KLD: 25.6182\n",
      "Epoch[5/20], Step [  200], loss: 105.2695 BCE 79.4780, KLD: 25.7915\n",
      "Epoch[5/20], Step [  300], loss: 100.0122 BCE 74.7255, KLD: 25.2867\n",
      "Epoch[5/20], Step [  400], loss: 105.3118 BCE 79.4509, KLD: 25.8609\n",
      "Epoch[5/20], Step [  500], loss: 93.3620 BCE 69.6605, KLD: 23.7015\n",
      "Epoch[5/20], Step [  600], loss: 100.8867 BCE 75.5573, KLD: 25.3295\n",
      "Epoch[5/20], Step [  700], loss: 102.5580 BCE 76.9864, KLD: 25.5717\n",
      "== Epoch[5/20]: elbo on valid set: -103.1253 ==\n",
      "\n",
      "Epoch[6/20], Step [  100], loss: 104.8680 BCE 78.8661, KLD: 26.0019\n",
      "Epoch[6/20], Step [  200], loss: 100.8545 BCE 74.9792, KLD: 25.8753\n",
      "Epoch[6/20], Step [  300], loss: 102.0934 BCE 76.4075, KLD: 25.6859\n",
      "Epoch[6/20], Step [  400], loss: 103.9941 BCE 77.5302, KLD: 26.4639\n",
      "Epoch[6/20], Step [  500], loss: 104.1007 BCE 78.4925, KLD: 25.6082\n",
      "Epoch[6/20], Step [  600], loss: 102.8741 BCE 76.1946, KLD: 26.6794\n",
      "Epoch[6/20], Step [  700], loss: 97.2958 BCE 71.4154, KLD: 25.8804\n",
      "== Epoch[6/20]: elbo on valid set: -101.9110 ==\n",
      "\n",
      "Epoch[7/20], Step [  100], loss: 103.7038 BCE 77.3687, KLD: 26.3352\n",
      "Epoch[7/20], Step [  200], loss: 106.3564 BCE 80.3319, KLD: 26.0245\n",
      "Epoch[7/20], Step [  300], loss: 100.2278 BCE 74.4580, KLD: 25.7699\n",
      "Epoch[7/20], Step [  400], loss: 105.9156 BCE 79.1726, KLD: 26.7430\n",
      "Epoch[7/20], Step [  500], loss: 100.5158 BCE 74.7540, KLD: 25.7619\n",
      "Epoch[7/20], Step [  600], loss: 97.1689 BCE 71.7660, KLD: 25.4029\n",
      "Epoch[7/20], Step [  700], loss: 103.8064 BCE 77.7159, KLD: 26.0905\n",
      "== Epoch[7/20]: elbo on valid set: -100.2393 ==\n",
      "\n",
      "Epoch[8/20], Step [  100], loss: 99.0235 BCE 73.3682, KLD: 25.6552\n",
      "Epoch[8/20], Step [  200], loss: 103.3346 BCE 76.1461, KLD: 27.1885\n",
      "Epoch[8/20], Step [  300], loss: 93.5548 BCE 68.1028, KLD: 25.4520\n",
      "Epoch[8/20], Step [  400], loss: 97.3768 BCE 71.0767, KLD: 26.3001\n",
      "Epoch[8/20], Step [  500], loss: 101.1765 BCE 75.3625, KLD: 25.8140\n",
      "Epoch[8/20], Step [  600], loss: 100.3000 BCE 73.8935, KLD: 26.4066\n",
      "Epoch[8/20], Step [  700], loss: 101.5074 BCE 75.3138, KLD: 26.1936\n",
      "== Epoch[8/20]: elbo on valid set: -99.2310 ==\n",
      "\n",
      "Epoch[9/20], Step [  100], loss: 97.0312 BCE 71.4838, KLD: 25.5474\n",
      "Epoch[9/20], Step [  200], loss: 104.5421 BCE 77.6678, KLD: 26.8743\n",
      "Epoch[9/20], Step [  300], loss: 98.0773 BCE 72.8184, KLD: 25.2589\n",
      "Epoch[9/20], Step [  400], loss: 103.1091 BCE 76.5272, KLD: 26.5819\n",
      "Epoch[9/20], Step [  500], loss: 99.8070 BCE 72.6779, KLD: 27.1290\n",
      "Epoch[9/20], Step [  600], loss: 98.7802 BCE 72.2343, KLD: 26.5459\n",
      "Epoch[9/20], Step [  700], loss: 99.2674 BCE 72.2189, KLD: 27.0485\n",
      "== Epoch[9/20]: elbo on valid set: -98.8106 ==\n",
      "\n",
      "Epoch[10/20], Step [  100], loss: 105.2606 BCE 78.8616, KLD: 26.3990\n",
      "Epoch[10/20], Step [  200], loss: 99.8770 BCE 73.2296, KLD: 26.6474\n",
      "Epoch[10/20], Step [  300], loss: 100.1407 BCE 73.7621, KLD: 26.3786\n",
      "Epoch[10/20], Step [  400], loss: 96.3982 BCE 70.9942, KLD: 25.4040\n",
      "Epoch[10/20], Step [  500], loss: 100.3213 BCE 74.1166, KLD: 26.2047\n",
      "Epoch[10/20], Step [  600], loss: 93.5560 BCE 67.6837, KLD: 25.8723\n",
      "Epoch[10/20], Step [  700], loss: 97.5962 BCE 71.5528, KLD: 26.0434\n",
      "== Epoch[10/20]: elbo on valid set: -97.5465 ==\n",
      "\n",
      "Epoch[11/20], Step [  100], loss: 102.0653 BCE 74.0243, KLD: 28.0410\n",
      "Epoch[11/20], Step [  200], loss: 100.6365 BCE 73.9524, KLD: 26.6841\n",
      "Epoch[11/20], Step [  300], loss: 93.9465 BCE 68.6832, KLD: 25.2633\n",
      "Epoch[11/20], Step [  400], loss: 93.3590 BCE 68.3236, KLD: 25.0353\n",
      "Epoch[11/20], Step [  500], loss: 98.2492 BCE 71.2024, KLD: 27.0468\n",
      "Epoch[11/20], Step [  600], loss: 94.8384 BCE 68.7513, KLD: 26.0870\n",
      "Epoch[11/20], Step [  700], loss: 96.9988 BCE 70.5117, KLD: 26.4871\n",
      "== Epoch[11/20]: elbo on valid set: -97.2294 ==\n",
      "\n",
      "Epoch[12/20], Step [  100], loss: 95.2556 BCE 69.6371, KLD: 25.6185\n",
      "Epoch[12/20], Step [  200], loss: 96.5313 BCE 70.1235, KLD: 26.4078\n",
      "Epoch[12/20], Step [  300], loss: 98.9043 BCE 72.1544, KLD: 26.7499\n",
      "Epoch[12/20], Step [  400], loss: 96.1127 BCE 69.8804, KLD: 26.2323\n",
      "Epoch[12/20], Step [  500], loss: 96.0189 BCE 69.5300, KLD: 26.4889\n",
      "Epoch[12/20], Step [  600], loss: 95.9415 BCE 69.7436, KLD: 26.1978\n",
      "Epoch[12/20], Step [  700], loss: 99.7314 BCE 72.7108, KLD: 27.0206\n",
      "== Epoch[12/20]: elbo on valid set: -96.8237 ==\n",
      "\n",
      "Epoch[13/20], Step [  100], loss: 94.9793 BCE 68.8340, KLD: 26.1453\n",
      "Epoch[13/20], Step [  200], loss: 99.1806 BCE 72.3094, KLD: 26.8712\n",
      "Epoch[13/20], Step [  300], loss: 95.7030 BCE 68.9426, KLD: 26.7604\n",
      "Epoch[13/20], Step [  400], loss: 94.8507 BCE 69.0107, KLD: 25.8400\n",
      "Epoch[13/20], Step [  500], loss: 100.3979 BCE 73.1244, KLD: 27.2735\n",
      "Epoch[13/20], Step [  600], loss: 97.3208 BCE 70.1706, KLD: 27.1502\n",
      "Epoch[13/20], Step [  700], loss: 100.3117 BCE 73.5670, KLD: 26.7447\n",
      "== Epoch[13/20]: elbo on valid set: -96.7963 ==\n",
      "\n",
      "Epoch[14/20], Step [  100], loss: 95.6720 BCE 68.7316, KLD: 26.9405\n",
      "Epoch[14/20], Step [  200], loss: 97.5021 BCE 70.9544, KLD: 26.5477\n",
      "Epoch[14/20], Step [  300], loss: 95.1048 BCE 68.7248, KLD: 26.3800\n",
      "Epoch[14/20], Step [  400], loss: 97.2033 BCE 70.4011, KLD: 26.8022\n",
      "Epoch[14/20], Step [  500], loss: 97.1063 BCE 70.6171, KLD: 26.4892\n",
      "Epoch[14/20], Step [  600], loss: 92.0818 BCE 66.2259, KLD: 25.8559\n",
      "Epoch[14/20], Step [  700], loss: 94.7626 BCE 68.3509, KLD: 26.4117\n",
      "== Epoch[14/20]: elbo on valid set: -95.6694 ==\n",
      "\n",
      "Epoch[15/20], Step [  100], loss: 92.6516 BCE 66.5816, KLD: 26.0700\n",
      "Epoch[15/20], Step [  200], loss: 91.0189 BCE 65.6932, KLD: 25.3256\n",
      "Epoch[15/20], Step [  300], loss: 94.7320 BCE 68.7208, KLD: 26.0112\n",
      "Epoch[15/20], Step [  400], loss: 97.5403 BCE 69.9814, KLD: 27.5589\n",
      "Epoch[15/20], Step [  500], loss: 95.8043 BCE 69.3581, KLD: 26.4461\n",
      "Epoch[15/20], Step [  600], loss: 100.1748 BCE 72.5317, KLD: 27.6431\n",
      "Epoch[15/20], Step [  700], loss: 93.7834 BCE 67.0780, KLD: 26.7054\n",
      "== Epoch[15/20]: elbo on valid set: -95.5165 ==\n",
      "\n",
      "Epoch[16/20], Step [  100], loss: 96.4192 BCE 69.5781, KLD: 26.8410\n",
      "Epoch[16/20], Step [  200], loss: 95.6875 BCE 68.9444, KLD: 26.7432\n",
      "Epoch[16/20], Step [  300], loss: 92.6194 BCE 66.9843, KLD: 25.6351\n",
      "Epoch[16/20], Step [  400], loss: 92.9096 BCE 66.2937, KLD: 26.6159\n",
      "Epoch[16/20], Step [  500], loss: 95.1901 BCE 69.3239, KLD: 25.8662\n",
      "Epoch[16/20], Step [  600], loss: 96.2348 BCE 69.2513, KLD: 26.9835\n",
      "Epoch[16/20], Step [  700], loss: 93.4019 BCE 66.9409, KLD: 26.4610\n",
      "== Epoch[16/20]: elbo on valid set: -95.2680 ==\n",
      "\n",
      "Epoch[17/20], Step [  100], loss: 91.8367 BCE 65.7944, KLD: 26.0423\n",
      "Epoch[17/20], Step [  200], loss: 93.3691 BCE 67.5666, KLD: 25.8026\n",
      "Epoch[17/20], Step [  300], loss: 93.7097 BCE 67.4063, KLD: 26.3034\n",
      "Epoch[17/20], Step [  400], loss: 96.2380 BCE 69.9418, KLD: 26.2961\n",
      "Epoch[17/20], Step [  500], loss: 96.6677 BCE 69.1422, KLD: 27.5255\n",
      "Epoch[17/20], Step [  600], loss: 94.0520 BCE 68.0939, KLD: 25.9581\n",
      "Epoch[17/20], Step [  700], loss: 96.5876 BCE 70.3962, KLD: 26.1914\n",
      "== Epoch[17/20]: elbo on valid set: -94.9772 ==\n",
      "\n",
      "Epoch[18/20], Step [  100], loss: 93.8262 BCE 67.2762, KLD: 26.5500\n",
      "Epoch[18/20], Step [  200], loss: 93.1020 BCE 66.9756, KLD: 26.1264\n",
      "Epoch[18/20], Step [  300], loss: 94.7907 BCE 67.6762, KLD: 27.1145\n",
      "Epoch[18/20], Step [  400], loss: 90.8313 BCE 64.9775, KLD: 25.8538\n",
      "Epoch[18/20], Step [  500], loss: 95.5168 BCE 69.0480, KLD: 26.4687\n",
      "Epoch[18/20], Step [  600], loss: 95.5038 BCE 68.6259, KLD: 26.8779\n",
      "Epoch[18/20], Step [  700], loss: 96.5749 BCE 69.9240, KLD: 26.6508\n",
      "== Epoch[18/20]: elbo on valid set: -95.0950 ==\n",
      "\n",
      "Epoch[19/20], Step [  100], loss: 91.4979 BCE 65.2597, KLD: 26.2382\n",
      "Epoch[19/20], Step [  200], loss: 96.8765 BCE 70.3962, KLD: 26.4802\n",
      "Epoch[19/20], Step [  300], loss: 97.3275 BCE 71.0656, KLD: 26.2619\n",
      "Epoch[19/20], Step [  400], loss: 97.4194 BCE 69.5326, KLD: 27.8868\n",
      "Epoch[19/20], Step [  500], loss: 92.7407 BCE 67.0098, KLD: 25.7309\n",
      "Epoch[19/20], Step [  600], loss: 90.4189 BCE 64.5171, KLD: 25.9018\n",
      "Epoch[19/20], Step [  700], loss: 95.2606 BCE 68.2901, KLD: 26.9706\n",
      "== Epoch[19/20]: elbo on valid set: -94.6460 ==\n",
      "\n",
      "Epoch[20/20], Step [  100], loss: 92.2222 BCE 66.2569, KLD: 25.9652\n",
      "Epoch[20/20], Step [  200], loss: 92.1063 BCE 66.0813, KLD: 26.0251\n",
      "Epoch[20/20], Step [  300], loss: 89.1416 BCE 63.4079, KLD: 25.7336\n",
      "Epoch[20/20], Step [  400], loss: 94.4981 BCE 68.5474, KLD: 25.9507\n",
      "Epoch[20/20], Step [  500], loss: 95.9097 BCE 68.6928, KLD: 27.2170\n",
      "Epoch[20/20], Step [  600], loss: 95.7891 BCE 68.7154, KLD: 27.0737\n",
      "Epoch[20/20], Step [  700], loss: 91.6536 BCE 66.2209, KLD: 25.4327\n",
      "== Epoch[20/20]: elbo on valid set: -94.3153 ==\n",
      "\n"
     ]
    }
   ],
   "source": [
    "batch_size = 64\n",
    "z_dim = 100\n",
    "vae = VAE(z_dim = z_dim)\n",
    "if cuda:\n",
    "    vae.cuda()\n",
    "    \n",
    "optimizer = torch.optim.Adam(vae.parameters(), lr = 3e-4)\n",
    "\n",
    "train_loader = data_loader(train_data, batch_size = batch_size)\n",
    "valid_loader = data_loader(valid_data, batch_size = batch_size)\n",
    "test_loader =  data_loader(test_data, batch_size = batch_size)\n",
    "\n",
    "max_epochs = 20\n",
    "\n",
    "for epoch in range(max_epochs):\n",
    "    vae.train()\n",
    "    for idx, x in enumerate(train_loader):\n",
    "        recon_x, mu, log_var = vae(x)\n",
    "        \n",
    "        # both methods work\n",
    "        #loss, bce, kld = compute_loss(recon_x, x, mu, log_var)\n",
    "        elbo, bce, kld = compute_elbo(recon_x, x, mu, log_var)\n",
    "        loss = -1 * elbo\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if (idx+1) % 100 == 0:\n",
    "            print (\"Epoch[{}/{}], Step [{:5}], loss: {:.4f} BCE {:.4f}, KLD: {:.4f}\"\n",
    "                   .format(epoch+1, max_epochs, idx+1, \n",
    "                           -elbo.item(), bce.item(), kld.item()))\n",
    "     \n",
    "    vae.eval()\n",
    "    val_elbo, val_loss, val_size = 0.0, 0.0, 0\n",
    "    #val_size = len(valid_loader)\n",
    "    for idx, x in enumerate(valid_loader):\n",
    "        batch_size = x.shape[0]\n",
    "        recon_x, mu, log_var = vae(x)\n",
    "        loss, _, _ = compute_loss(recon_x, x, mu, log_var)\n",
    "        elbo, _, _ = compute_elbo(recon_x, x, mu, log_var)\n",
    "        val_elbo += elbo.item() * batch_size\n",
    "        val_loss += loss.item() * batch_size\n",
    "        val_size += batch_size\n",
    "        \n",
    "    val_loss /= val_size\n",
    "    val_elbo /= val_size\n",
    "    print(\"== Epoch[{}/{}]: elbo on valid set: {:.4f} ==\\n\"\n",
    "          .format(epoch+1, max_epochs, val_elbo))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PX4eGZW310EU"
   },
   "source": [
    "**Save model parameters**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wX9qrWmPC3Iu"
   },
   "outputs": [],
   "source": [
    "import os.path\n",
    "\n",
    "if os.path.isfile('params.pkl'):\n",
    "    os.remove('params.pkl') \n",
    "    \n",
    "torch.save(vae.state_dict(),  'params.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wyJ3VjRw17Kz"
   },
   "source": [
    "**Evaluating**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9nlDCoohOhm0"
   },
   "outputs": [],
   "source": [
    "from torch.distributions.normal import Normal\n",
    "\n",
    "#M = 50  #batch_size\n",
    "D, K, L = 784, 200, 100\n",
    "\n",
    "def sampling(mu, log_var, k = K):\n",
    "    sigma = torch.exp(0.5 * log_var)\n",
    "    m = Normal(mu, sigma)\n",
    "    z = torch.squeeze(m.sample(torch.Size([k])), 1)\n",
    "    return z\n",
    "\n",
    "def log_normal_densities(z, mu, log_var):\n",
    "    log_prob_density = None\n",
    "    sigma = torch.exp(0.5 * log_var)\n",
    "    m = Normal(mu, sigma)\n",
    "    log_probs = m.log_prob(z)\n",
    "    return torch.sum(log_probs, dim = 1, keepdim = True) # [K, 1]\n",
    "\n",
    "def log_probs_reconstruct(decoder, x_i, z):\n",
    "    k = z.size(0)\n",
    "    recon_xs_i = decoder(z).view(k, -1)            # flatten [K, 784]\n",
    "    recon_xs_i = torch.sigmoid(recon_xs_i)      # conver to probabilities\n",
    "    x_i = x_i.view(1, -1)\n",
    "    xs_i = x_i.repeat(k, 1)                                  # copy original x to K times\n",
    "    # compare all 784 \n",
    "    log_p_theta = torch.log(recon_xs_i * xs_i + (1- recon_xs_i) * (1-xs_i))\n",
    "    return torch.sum(log_p_theta, dim = 1, keepdim = True)  # [K, 1]\n",
    "\n",
    "def generate_Z(encoder, x, k = K):\n",
    "    M = x.size(0)\n",
    "    x = x.view(M, 1, 28, 28)\n",
    "    mu, log_var = encoder(x)     # [batch_size, L]\n",
    "    L = mu.size(1)\n",
    "    Z = sampling(mu, log_var, k)\n",
    "    Z = torch.transpose(Z, 0, 1)\n",
    "    return Z\n",
    "    \n",
    "def evaluate_log_likelihood(model, x, Z):\n",
    "    encoder = model.encoder\n",
    "    decoder = model.decoder\n",
    "    mini_batch_size = x.size(0) \n",
    "    x = x.view(mini_batch_size, -1) \n",
    "    K = Z.size(1)          \n",
    "    log_p = [0 for i in range(mini_batch_size)]\n",
    "    for i in range(mini_batch_size):\n",
    "        x_i = x[i, :]          # [M, D]\n",
    "        z = Z[i, :, :]          # [K, L]\n",
    "        mu, log_var = encoder(x_i.view(1, 1, 28, 28))    # [1, L], [1, L]\n",
    "        mu0, log_var0 = torch.zeros(1, L), torch.zeros(1, L)\n",
    "        if cuda:\n",
    "            mu0 = mu0.cuda()\n",
    "            log_var0 = log_var0.cuda()\n",
    "        log_p_z = log_normal_densities(z, mu0, log_var0) \n",
    "        log_q_z = log_normal_densities(z, mu, log_var) \n",
    "        log_p_theta_x_i = log_probs_reconstruct(decoder, x_i, z) \n",
    "        \n",
    "        # LogSumExp trick for underflow\n",
    "        log_p_for_sum = log_p_theta_x_i + log_p_z - log_q_z\n",
    "        max_log_p = torch.max(log_p_for_sum)\n",
    "        log_p_for_sum1 = log_p_for_sum - max_log_p\n",
    "        log_1k = torch.log(torch.tensor([1./K]))\n",
    "        log_p_xi = log_1k + max_log_p + torch.log(torch.sum(torch.exp(log_p_for_sum1)))\n",
    "        log_p[i] = log_p_xi.item()\n",
    "    \n",
    "    return log_p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PTgEyFhElUGh"
   },
   "outputs": [],
   "source": [
    "encoder = vae.encoder\n",
    "decoder = vae.decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 257241,
     "status": "ok",
     "timestamp": 1555732369681,
     "user": {
      "displayName": "Lifeng W",
      "photoUrl": "",
      "userId": "04920199623700226835"
     },
     "user_tz": 240
    },
    "id": "d9IP0xbBliLP",
    "outputId": "72602fdd-79f1-45bf-d908-657dc9735c96"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([16, 1, 28, 28])\n",
      "torch.Size([16, 200, 100])\n"
     ]
    }
   ],
   "source": [
    "print(x.shape)\n",
    "Z = generate_Z(encoder, x, k = 200)\n",
    "print(Z.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "iwHGZa43Xu3m"
   },
   "source": [
    "**Evaluate with ELBO**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "W6J0lyqIXtVh"
   },
   "outputs": [],
   "source": [
    "model = VAE(z_dim = z_dim)\n",
    "if cuda:\n",
    "    model.cuda()\n",
    "model.load_state_dict(torch.load('params.pkl'))\n",
    "model.eval()\n",
    "\n",
    "def calculate_elbo(model, data_loader):\n",
    "    data_elbo, data_loss, data_size = 0.0, 0.0, 0\n",
    "    for idx, x in enumerate(data_loader):\n",
    "        batch_size = x.shape[0]\n",
    "        recon_x, mu, log_var = model(x)\n",
    "        loss, _, _ = compute_loss(recon_x, x, mu, log_var)\n",
    "        elbo, _, _ = compute_elbo(recon_x, x, mu, log_var)\n",
    "        data_elbo += elbo.item() * batch_size\n",
    "        data_loss += loss.item() * batch_size\n",
    "        data_size += batch_size\n",
    "        \n",
    "    data_loss /= data_size\n",
    "    data_elbo /= data_size\n",
    "    return data_elbo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 85
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 258472,
     "status": "ok",
     "timestamp": 1555732370935,
     "user": {
      "displayName": "Lifeng W",
      "photoUrl": "",
      "userId": "04920199623700226835"
     },
     "user_tz": 240
    },
    "id": "aLRxMPakRIfF",
    "outputId": "1867a882-e01c-45fc-d781-c9d6a3879196"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/upsampling.py:129: UserWarning: nn.UpsamplingBilinear2d is deprecated. Use nn.functional.interpolate instead.\n",
      "  warnings.warn(\"nn.{} is deprecated. Use nn.functional.interpolate instead.\".format(self.name))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "elbo on valid set =  -94.33704245605469\n",
      "elbo on test set =  -93.69906392822266\n"
     ]
    }
   ],
   "source": [
    "print(\"elbo on valid set = \", calculate_elbo(model, valid_loader))\n",
    "print(\"elbo on test set = \", calculate_elbo(model, test_loader))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LAwc3DsGX38v"
   },
   "source": [
    "**Evaluate with log-likelihood**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5N3Cpwb-8oYu"
   },
   "outputs": [],
   "source": [
    "def  evaluate(model, data_loader):\n",
    "    log_px = []\n",
    "    \n",
    "    for idx, x in enumerate(test_loader):\n",
    "        Z = generate_Z(model.encoder, x, k = 200)\n",
    "        log_px.extend(evaluate_log_likelihood(model, x, Z))\n",
    "\n",
    "    mean_log_px = 0.0\n",
    "    for i in range(len(log_px)):\n",
    "        mean_log_px += log_px[i]\n",
    "    mean_log_px /= len(log_px)\n",
    "    \n",
    "    return mean_log_px, log_px"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 85
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 467763,
     "status": "ok",
     "timestamp": 1555732580249,
     "user": {
      "displayName": "Lifeng W",
      "photoUrl": "",
      "userId": "04920199623700226835"
     },
     "user_tz": 240
    },
    "id": "iudZbp4LLVWH",
    "outputId": "5dc23e37-8ac0-43a8-fb4d-f1be0d6da7c4"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/upsampling.py:129: UserWarning: nn.UpsamplingBilinear2d is deprecated. Use nn.functional.interpolate instead.\n",
      "  warnings.warn(\"nn.{} is deprecated. Use nn.functional.interpolate instead.\".format(self.name))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean log-likelihood on valid set =  -88.38922895717621\n",
      "Mean log-likelihood on test set =  -88.38320319671631\n"
     ]
    }
   ],
   "source": [
    "#valid_loader = data_loader(valid_data, batch_size = batch_size)\n",
    "#test_loader =  data_loader(test_data, batch_size = batch_size)\n",
    "\n",
    "valid_mean_log_px,  valid_log_px = evaluate(model, valid_loader)\n",
    "test_mean_log_px,  test_log_px = evaluate(model, test_loader)\n",
    "\n",
    "#print(valid_log_px)\n",
    "print(\"Mean log-likelihood on valid set = \", valid_mean_log_px)\n",
    "#print(test_log_px)\n",
    "print(\"Mean log-likelihood on test set = \", test_mean_log_px)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "hw3_2_0419_from_qiang.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
