{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"prob1_hw1.ipynb","version":"0.3.2","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"metadata":{"id":"yoj3v7b5Nciw","colab_type":"text"},"cell_type":"markdown","source":["# Practical Part of Assignment 1 - IFT6135-H19\n","\n","**Team member**: Qiang Ye (20139927)"]},{"metadata":{"id":"X3c-h5FjOQEw","colab_type":"text"},"cell_type":"markdown","source":["## Support for Computation on Google Colab"]},{"metadata":{"id":"IvGFIzH4_wnQ","colab_type":"code","outputId":"1b4595ef-ea40-4306-dda9-384a83e2096a","executionInfo":{"status":"ok","timestamp":1550181393560,"user_tz":300,"elapsed":3509,"user":{"displayName":"Qiang YE","photoUrl":"https://lh6.googleusercontent.com/-k-9mYNphfpc/AAAAAAAAAAI/AAAAAAAAARQ/zyID3sFntfM/s64/photo.jpg","userId":"15200210150486633975"}},"colab":{"base_uri":"https://localhost:8080/","height":35}},"cell_type":"code","source":["from os.path import exists\n","from google.colab import drive\n","import os\n","drive.mount('/content/dirve/', force_remount = True)\n"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Mounted at /content/dirve/\n"],"name":"stdout"}]},{"metadata":{"id":"upqtlSNSA7jP","colab_type":"code","colab":{}},"cell_type":"code","source":["os.chdir(\"dirve/My Drive/Colab Notebooks/IFT6135/hw1/\")\n","#!ls -al"],"execution_count":0,"outputs":[]},{"metadata":{"id":"-trqNYyA0opF","colab_type":"text"},"cell_type":"markdown","source":["## Problem 1\n","\n","### Building the Model\n","\n","#### 1. Possible hidden dims settings for number of parameters fallen into [0.5M, 1M]\n","\n","Consider a MLP with 2 hidden layers with $h^{(1)}$ and $h^{(2)}$ hidden units, an input layer with feature number of $h^{(0)} = 784$ and an output layer with number of labels $h^{(3)} = 10$,  the total number of parameters($\\#P\\_total$) is:\n","\n","\\begin{align*}\n","\\#P\\_total &= \\#P\\_hidden1 + \\#P\\_hidden2 + \\#P\\_output\\\\\n","&= h^{(1)}  (h^{(0)} + 1) + h^{(2)}(h^{(1)} + 1) + h^{(3)}(h^{(2)}+1)\\\\\n","\\end{align*}"]},{"metadata":{"id":"AHItzlBZ3R7N","colab_type":"code","colab":{}},"cell_type":"code","source":["def num_parameters(h1, h2, h0 = 784, h3 = 10):\n","    p_hidden1 = h1 * (h0 + 1)\n","    p_hidden2 = h2 * (h1 + 1)\n","    p_output = h3 * (h2 + 1)\n","    p_total = p_hidden1 + p_hidden2 + p_output\n","    return p_total"],"execution_count":0,"outputs":[]},{"metadata":{"id":"Y_kMGKdr3roG","colab_type":"code","outputId":"984f4437-94ea-4c1e-f60f-0d28a36cd052","executionInfo":{"status":"ok","timestamp":1550181402708,"user_tz":300,"elapsed":963,"user":{"displayName":"Qiang YE","photoUrl":"https://lh6.googleusercontent.com/-k-9mYNphfpc/AAAAAAAAAAI/AAAAAAAAARQ/zyID3sFntfM/s64/photo.jpg","userId":"15200210150486633975"}},"colab":{"base_uri":"https://localhost:8080/","height":143}},"cell_type":"code","source":["dim_options = [128, 256, 512, 1024, 2048, 4096]\n","print(\"Possible values of (h1, h2):\")\n","\n","for h1 in dim_options:\n","    for h2 in dim_options:\n","        if 5e5 <= num_parameters(h1, h2) <= 1e6:\n","            print(\"({}, {})\".format(h1, h2))"],"execution_count":4,"outputs":[{"output_type":"stream","text":["Possible values of (h1, h2):\n","(128, 4096)\n","(256, 2048)\n","(512, 256)\n","(512, 512)\n","(512, 1024)\n","(1024, 128)\n"],"name":"stdout"}]},{"metadata":{"id":"VWKYy5dV55Jd","colab_type":"text"},"cell_type":"markdown","source":["#### 2. Implementation of a MLP class"]},{"metadata":{"id":"_Skm84RoxQku","colab_type":"code","colab":{}},"cell_type":"code","source":["%matplotlib inline\n","from utils import print_progress\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import pickle\n","import gzip\n","import time"],"execution_count":0,"outputs":[]},{"metadata":{"id":"FRzRvRTWxY-V","colab_type":"code","colab":{}},"cell_type":"code","source":["class NN(object):\n","    def __init__(self, \n","                 #hidden_dims = (512, 256), \n","                 #n_hidden = None, \n","                 learning_rate = 1e-4,\n","                 max_epochs = 10,\n","                 batch_size = 1, # stochastic gradient descent\n","                 input_dim = 784,\n","                 output_dim = 10,\n","                 activate_func = \"ReLU\",\n","                 early_stop = False,\n","                 mode = 'train', \n","                 #weight_init_mode = 'Glorot',\n","                 data_path = None, \n","                 model_path = None,\n","                 verbose = False,\n","                 #random_seed = 0\n","                ):\n","        self.input_dim = input_dim\n","        self.output_dim = output_dim\n","        self.hidden_dims = None # hidden_dims\n","        self.dims = None # self._generate_dims()\n","        self.n_hidden = None\n","        #if type(hidden_dims) in [list, tuple]:\n","        #    self.n_hidden = len(hidden_dims)\n","        #else:\n","        #    self.n_hidden = n_hidden # number of hidden layers\n","        \n","        # Ws[0],bs[0] always be None\n","        # Ws[1] connects inputlayer to 1st hidden layer\n","        # bs[1] is the bias of 1st hidden layer\n","        self.Ws = None #[None] * (self.n_hidden + 2) \n","        self.bs = None #[None] * (self.n_hidden + 2) \n","\n","        self.activate_func = activate_func # nonlinear activate function\n","        self.lr = learning_rate\n","        self.max_epochs = max_epochs\n","        self.batch_size = batch_size\n","        self.early_stop = early_stop\n","        self.verbose = verbose\n","        self.mode = mode\n","        self.data_path = data_path\n","        self.model_path = model_path\n","        self.random_seed = None # random_seed\n","        \n","        self.weight_init_mode = None # weight_init_mode\n","        return\n","    \n","    \n","    def _generate_dims(self):\n","        \"\"\"generate the nn layer dims, including the input and output layer\n","        \"\"\"\n","        dims = [self.input_dim]\n","        dims.extend(list(self.hidden_dims))\n","        dims.extend([self.output_dim])\n","        return dims\n","    \n","        \n","    def load_data(self, dataset_type = \"train\"):\n","        \"\"\"load specified dataset from data_path\n","        Params\n","            dataset_type: specify the type of the dataset, str. Example: \"train\"\n","            ,\"valid\", \"test\"\n","        Returns\n","            dataset, tuple (X, y) where shape of X is (sample_size, n_feature), \n","            shape of y is (sample_size, )\n","        \"\"\"\n","        with gzip.open(self.data_path,'rb') as f:\n","            u = pickle._Unpickler(f)\n","            u.encoding = 'latin1'\n","            data_set = u.load()\n","            # all _data are tuple with 2 elements: X, and y with shape like\n","            # (50000, 784) and (50000, ) respectively\n","        dataset_type = dataset_type.upper()\n","        if dataset_type == \"TRAIN\":\n","            return data_set[0]\n","        elif dataset_type == \"VALID\":\n","            return data_set[1]\n","        else: # test\n","            return data_set[2]\n","    \n","        \n","    def initialize_weights(self, hidden_dims, n_hidden = None,\n","                           init_mode = 'Glorot', random_seed = 0):\n","        \"\"\"initialize weights and bias according to the attributes given.\n","        \"\"\"\n","        self.hidden_dims = hidden_dims\n","        if type(hidden_dims) in [list, tuple]:\n","            self.n_hidden = len(hidden_dims)\n","        else:\n","            self.n_hidden = n_hidden # number of hidden layers\n","        \n","        self.dims = self._generate_dims()\n","        self.random_seed = random_seed\n","        self.weight_init_mode = init_mode\n","        \n","        self.Ws = [None] * (self.n_hidden + 2)\n","        self.bs = [None] * (self.n_hidden + 2)\n","        \n","        np.random.seed(self.random_seed)\n","        dims = self.dims\n","        \n","        for i in range(1, len(dims)): \n","            init_mode = init_mode.upper()\n","            weight_shape = (dims[i], dims[i-1])\n","            if init_mode == \"ZERO\":\n","                self.Ws[i] = np.zeros(weight_shape)\n","            elif init_mode == \"NORMAL\":\n","                self.Ws[i] = np.random.normal(0, 1, weight_shape)\n","            else: # â€˜Glorot'\n","                high = float(np.sqrt(6.0/(dims[i] + dims[i-1])))\n","                self.Ws[i] = np.random.uniform(-1 * high, high, weight_shape)\n","\n","            self.bs[i] = np.zeros((dims[i], 1))\n","                    \n","        return\n","    \n","    \n","    def zero_grad(self):\n","        \"\"\"prepare zero grad of weight and bias parameters\n","        Params\n","            None\n","        Returns\n","            grad_Ws, grad_bs\n","        \"\"\"\n","        grad_Ws = [None] * (self.n_hidden + 2) # grad_Ws[0] always None\n","        grad_bs = [None] * (self.n_hidden + 2) # grad_bs[0] always None\n","        for i in range(1, self.n_hidden+2):\n","            grad_Ws[i] = np.zeros_like(self.Ws[i])\n","            grad_bs[i] = np.zeros_like(self.bs[i])\n","        return grad_Ws, grad_bs\n","    \n","    \n","    def forward(self, X):\n","        \"\"\"forward process of the network\n","        Params\n","            X: input (batch_size, input_features)\n","        Returns\n","            cache: a dictionary of all intermediate and final output.\n","        \"\"\"\n","        h_x = X # h_x is output of the layer\n","        cache = {\"X\": X, \"h0\": X} # there's no \"a0\"\n","        for i in range(1, self.n_hidden + 2):\n","            # a_x is pre-activate value of a layer\n","            # h_x is output of a layer\n","            a_x = np.dot(h_x, self.Ws[i].T) + self.bs[i].T\n","            if i == self.n_hidden + 1: # output layer\n","                h_x = self.softmax(a_x)\n","                cache[\"ao\"] = a_x\n","                cache[\"ho\"] = h_x\n","            else: # hidden layer(s)\n","                h_x = self.activation(a_x, self.activate_func)\n","                cache[\"a\"+str(i)] = a_x\n","                cache[\"h\"+str(i)] = h_x\n","        \n","        return cache\n","    \n","    \n","    def activation(self, X, func = \"RELU\"):\n","        \"\"\"compute activation of a Tensor\n","        Params\n","            X \n","        \"\"\"\n","        func = func.upper() #self.activate_func.upper()\n","        if func == \"TANH\":\n","            e_pos = np.exp(X)\n","            e_neg = np.exp(-1 * X)\n","            return (e_pos - e_neg)/(e_pos + e_neg)\n","        elif func == \"SIGMOID\":\n","            return 1.0/(1 + np.exp(-1 * X))\n","        else:# func in ['ReLU', 'relu', 'RELU', 'Relu']:\n","            return np.maximum(X, 0)\n","        \n","        \n","    def _derivative(self, h, func = \"RELU\"):\n","        \"\"\" derivative of certain output of activation functions using \n","        output of activation function\n","        \"\"\"\n","        func = func.upper() #self.activate_func.upper()\n","        if func == 'TANH':\n","            return 1 - np.power(h, 2)\n","        elif func == 'SIGMOID':\n","            return np.multiply(h, 1-h)\n","        else:# func in ['ReLU', 'relu', 'RELU', 'Relu']:\n","            return np.sign(h)\n","        \n","    \n","    def loss(self, ho, labels):\n","        \"\"\"compute average loss\n","        params\n","            ho: output of network (batch_size, output_dim)\n","            labels: true label (batch_size, class_index)\n","        returns\n","            loss, average loss on batch_size samples, float\n","        \"\"\"\n","        #print(prediction)\n","        batch_size, output_dim = ho.shape\n","        #labels = labels.reshape(-1, 1)\n","        labels = labels.astype(int)\n","\n","        ho_labels_probs = ho[np.arange(batch_size), labels]\n","        # to avoid log(0)\n","        ho_labels_probs = np.maximum(ho_labels_probs, 1e-100)\n","        loss = np.mean(-np.log(ho_labels_probs))\n","        return float(loss)\n","    \n","    \n","    def accuracy(self, ho, labels):\n","        \"\"\"compute average accuracy\n","        params\n","            ho: output of network (batch_size, output_dim)\n","            labels: true label (batch_size, class_index)\n","        returns\n","            accuracy, average accuracy on batch_size samples, float\n","        \"\"\"\n","        batch_size, output_dim = ho.shape\n","        labels = labels.reshape(-1, )\n","        labels = labels.astype(int)\n","        ho_labels = np.argmax(ho, axis = 1)\n","        return float(np.sum(ho_labels == labels) / len(ho_labels))\n","\n","    \n","    def softmax(self, ao):\n","        \"\"\"return softmax of pre-activate value of final layer\n","        Params\n","            ao: pre-activate value of final layer, (batch_size, n_class)\n","        Returns\n","            probs: (batch_size, n_class)\n","        \"\"\"\n","        if len(ao.shape) == 1: # shape like (4,)\n","            ao = ao.reshape(1, -1)\n","        \n","        ao_max = np.max(ao, axis = 1, keepdims = True)\n","        sum_exp_ao = np.sum(np.exp(ao - ao_max), axis = 1, keepdims = True)\n","        return np.exp(ao - ao_max) / sum_exp_ao\n","    \n","    \n","    def _one_hot(self, output_dim, labels):\n","        labels = labels.reshape(-1, ) # (batch_size, )\n","        labels = labels.astype(int)\n","        batch_size = labels.shape[0]\n","        result = np.zeros((batch_size, output_dim))\n","        result[np.arange(batch_size), labels] = 1\n","        return result\n","    \n","    \n","    def backward(self, cache, labels):\n","        \"\"\"backward process of a neural network\n","        Params\n","            cache: output of function forward, a dictionary of intermediate \n","                output neural network\n","            labels:true label of dataset\n","        Returns\n","            (grads_Ws, grads_bs), tuple \n","        \"\"\"\n","        ao = cache[\"ao\"]\n","        ho = cache[\"ho\"]\n","        X = cache[\"X\"]\n","        grad_Ws, grad_bs = self.zero_grad()\n","        \n","        batch_size, output_dim = ho.shape\n","        # skip calculating grad_ho, we directly calculate grad_ao,\n","        # as grad_ao is easy to be calculated.\n","        grad_ao = ho - self._one_hot(output_dim, labels)\n","        \n","        i = self.n_hidden + 1 #  i = 3 if n_hidden = 2\n","        grad_Ws[i] = np.dot(grad_ao.T, cache[\"h\"+str(i-1)])\n","        grad_bs[i] = grad_ao\n","        \n","        grad_a = grad_ao\n","        for i in range(self.n_hidden, 0, -1):\n","            # a is pre-activate value of a (hidden) layer\n","            # h is a output of a layer.\n","            a = cache[\"a\"+str(i)] # pre-activation\n","            h = cache[\"h\"+str(i)] # activated output\n","            \n","            grad_h = np.dot(grad_a, self.Ws[i+1])\n","            grad_a = np.multiply(grad_h, self._derivative(h, self.activate_func))\n","            \n","            grad_Ws[i] = np.dot(grad_a.T, cache[\"h\"+str(i-1)])\n","            grad_bs[i] = grad_a\n","            \n","       \n","        for i in range(self.n_hidden+1, 0, -1):\n","            grad_Ws[i] /= batch_size\n","            grad_bs[i] = np.mean(grad_bs[i], axis = 0, keepdims = True).T\n","\n","        grads = (grad_Ws, grad_bs)\n","        return grads\n","    \n","    \n","    def update(self, grads, learning_rate = 1e-4):\n","        \"\"\"pudate parameters\n","        Params\n","            learning_rate: learning_rate, float\n","        Returns\n","            None\n","        \"\"\"\n","        lr = learning_rate\n","        for i in range(1, self.n_hidden + 2):\n","            self.Ws[i] -= lr * grads[0][i]\n","            self.bs[i] -= lr * grads[1][i]\n","        \n","        return\n","    \n","            \n","    def train(self, \n","              batch_size = 1,  \n","              learning_rate = 1e-3, \n","              activate_func = \"Relu\",\n","              show_live_progress = True,\n","              epoch_patience = 20,\n","              max_epochs = 10,\n","              early_stop = True,\n","              model_path = 'model_auto_save.pkl',\n","              auto_save_interval = 10\n","             ):\n","        \"\"\"training a neural network. at each epoch, training the model on \n","        training dataset, and then compute loss and accuracy on validate dataset\n","        Params\n","            batch_size: mini_batch_size, int\n","            learning_rate: learning_rate, float\n","            activate_func: activate function, str\n","            show_live_progress: if display live training progress, bool\n","            epoch_patience: patient epoch for early stopping, int\n","            max_epochs: max epochs for training, int\n","            early_stop: if early stopping is applicable, bool\n","        Returns\n","            (losses, accuracies, best_acc)\n","            losses: {\"train\":[], \"val\":[]} \n","            accuracies: {\"train\":[], \"val\":[]}\n","            best_acc: best accuracy on validate set.\n","        \"\"\"\n","        \n","        self.activate_func = activate_func\n","        #self.mode = mode\n","        self.max_epochs = max_epochs\n","        self.batch_size = batch_size\n","        self.lr = learning_rate\n","        self.early_stop = early_stop\n","        \n","        #print(\"traing hyper-parameters:\")\n","        print(\"hidden dims:{}\".format(self.hidden_dims), end = \" \")\n","        print(\"lr:{},\".format(self.lr), end = \" \")\n","        print(\"{},\".format(self.activate_func), end = \" \")\n","        print(\"batch size:{},\".format(self.batch_size))\n","        print(\"max epochs:{},\".format(self.max_epochs), end = \" \")\n","        print(\"early stopping:{},\".format(early_stop), end = \" \")\n","        print(\"patience:{}\".format(epoch_patience))\n","        \n","        best_acc = {'train':0.0, 'val':0.0} \n","        losses = {'train': [], 'val':[]}\n","        accuracies = {'train': [], 'val':[]}\n","        patience_used = 0         # patience used\n","        early_stopped = 0\n","        start = time.time()\n","        dataset = {\"train\": self.load_data(\"train\"),\n","                  \"val\": self.load_data(\"val\")}\n","        \n","        for epoch in range(self.max_epochs):\n","            since  = time.time()\n","            s_before = '[Epoch{:>3d}/{} '.format(epoch, self.max_epochs - 1)\n","            s_after = ']'\n","            if show_live_progress:\n","                print_progress(0, 0, s_before, s_after)\n","            else:\n","                # print(s_before + s_after, end = \"\")\n","                pass\n","            \n","            for mode in ['train', 'val']:\n","                since_mode = time.time()\n","                \n","                loss, acc = 0.0, 0.0\n","                if show_live_progress:\n","                    print_progress(0, 0, s_before, s_after)\n","                \n","                X, y = dataset[mode]\n","                # shuffle data during training \n","                if mode == \"train\":\n","                    data = np.concatenate((X, y.reshape(-1, 1)), axis = 1)\n","                    np.random.shuffle(data)\n","                    X, y = data[:,:-1], data[:,-1]\n","                    y = y.reshape(-1, )\n","                    \n","                # X (50000, 784), y (50000, )\n","                sample_size, _ = X.shape\n","                batches = int(np.ceil(sample_size / self.batch_size))\n","                \n","                for j in range(batches):\n","                    b_start = j * self.batch_size\n","                    b_end = min(sample_size, (j+1) * self.batch_size)\n","                    batch_X = X[b_start:b_end, :]\n","                    batch_y = y[b_start:b_end]\n","                    cache = self.forward(batch_X)\n","                    if mode == \"train\":\n","                        grads = self.backward(cache, batch_y)\n","                        self.update(grads, self.lr)\n","\n","                    ho = cache[\"ho\"]\n","                    #factor =  (b_end - b_start) / b_end\n","                    #loss += (self.loss(ho, batch_y) - loss) * factor\n","                    #acc += (self.accuracy(ho, batch_y) - acc) * factor\n","\n","                    loss += self.loss(ho, batch_y) * (b_end - b_start)\n","                    acc += self.accuracy(ho, batch_y) * (b_end - b_start)\n","                    if show_live_progress:\n","                        time_elapsed = time.time() - since_mode\n","                        print_progress(b_end/sample_size, \n","                                       time_elapsed, \n","                                       s_before, \n","                                       s_after)\n","                    # end batch loop\n","                \n","                loss /= sample_size\n","                acc /= sample_size\n","                \n","                losses[mode].append(loss)\n","                accuracies[mode].append(acc)\n","                time_elapsed = time.time() - since_mode\n","                \n","                s_after += \" {} loss: {:.4f}, accuracy: {:<7.2%}\".format(\n","                    mode, loss, acc)\n","                \n","                \n","                if acc > best_acc[mode]:\n","                    best_acc[mode] = acc\n","                    if mode == \"val\":\n","                        s_after += \" *\"\n","                        patience_used = 0\n","                else:\n","                    if mode == \"val\":\n","                        patience_used += 1\n","                        if early_stop and patience_used >= epoch_patience:\n","                            early_stopped = epoch - epoch_patience\n","\n","                if show_live_progress:\n","                    print_progress(1, time_elapsed, s_before, s_after)\n","                else:\n","                    #print(' {} loss: {:.4f} acc: {:<7.2%} '.format(\n","                    #    mode, loss, acc), end = \" \")   \n","                    pass\n","                # end mode loop\n","                \n","            time_elapsed = time.time() - since\n","            if show_live_progress:\n","                print_progress(1, time_elapsed, s_before, s_after)\n","            else:\n","                #print(' {:.0f}m{:.0f}s'.format(\n","                #    time_elapsed // 60, time_elapsed % 60), end = \"\") \n","                pass\n","            print()\n","\n","            if epoch % auto_save_interval == 0:\n","                self.save_model(model_path)\n","            \n","            if early_stopped > 0:\n","                print(\"Early stopped at epoch: {}\".format(early_stopped))\n","                break\n","            \n","            if best_acc['val'] >= 0.97:\n","                print(\"Accuracy on validate set arrives 0.97, stop training\")\n","                break\n","                \n","            # end epoch loop\n","                \n","        time_elapsed = time.time() - start\n","        print('Training complete in {:.0f}m {:.0f}s'.format(\n","            time_elapsed // 60, time_elapsed % 60))\n","        print('Best val Acc on val:   {:.2%}'.format(best_acc['val']))\n","                \n","        return (losses, accuracies, best_acc['val'])\n","    \n","    \n","    def test(self):\n","        # no need so far.\n","        pass\n","    \n","    def save_model(self, filename):\n","        with open(filename, 'wb') as output:  # Overwrites any existing file.\n","            pickle.dump(self, output, pickle.HIGHEST_PROTOCOL)\n","    \n","    def load_model(self, filename):\n","        with open(filename, 'rb') as file:\n","            self = pickle.load(file)\n","        \n","    def net_info(self):\n","        \"\"\"display basic net configurations, architecture, total parameters,\n","        etc.\n","        \"\"\"\n","        print(\"Activation function: {}\".format(self.activate_func))\n","        if self.dims is None:\n","            return\n","        print(\"{} hidden layers\".format(self.n_hidden))\n","        print(\"Mode of weghts initialization: {}\".format(self.weight_init_mode))\n","        if self.lr is not None:\n","            print(\"learning rate: {}\".format(self.lr))\n","        if self.activate_func is not None:\n","            print(\"activate function: {}\".format(self.activate_func))\n","        if self.batch_size is not None:\n","            print(\"mini_batch_size: {}\".format(self.batch_size))\n","        num_params = 0\n","        for i in range(len(self.dims)):\n","            print(\"Layer{}:{:>5} neurons.\".format(i, self.dims[i]), end = \" \")\n","            if i > 0:\n","                print(\"Weights:{}, Bias:{}\".format(\n","                    self.Ws[i].shape, self.bs[i].shape))\n","                num_params += self.dims[i] * (1 + self.dims[i-1])\n","            else:\n","                print(\"\")\n","        print(\"Total number of params:{}\".format(num_params))"],"execution_count":0,"outputs":[]},{"metadata":{"id":"IBnxgSDN5eZy","colab_type":"text"},"cell_type":"markdown","source":["### Initialization"]},{"metadata":{"id":"I6oDZh5PWgGD","colab_type":"text"},"cell_type":"markdown","source":["#### 1. Model architecture and training the model using 3 different weights initialization methods."]},{"metadata":{"id":"LAS5NIv9CLnk","colab_type":"code","outputId":"689c22cb-f6b6-4f51-b339-abb2f9412761","executionInfo":{"status":"ok","timestamp":1550181679197,"user_tz":300,"elapsed":632,"user":{"displayName":"Qiang YE","photoUrl":"https://lh6.googleusercontent.com/-k-9mYNphfpc/AAAAAAAAAAI/AAAAAAAAARQ/zyID3sFntfM/s64/photo.jpg","userId":"15200210150486633975"}},"colab":{"base_uri":"https://localhost:8080/","height":215}},"cell_type":"code","source":["max_epochs = 10\n","mlp = NN(data_path = 'mnist.pkl.gz', batch_size = 32,\n","         activate_func = \"ReLU\", max_epochs = max_epochs)\n","mlp.initialize_weights(hidden_dims = (512, 256), init_mode = \"Zero\")\n","mlp.net_info()"],"execution_count":12,"outputs":[{"output_type":"stream","text":["Activation function: ReLU\n","2 hidden layers\n","Mode of weghts initialization: Zero\n","learning rate: 0.0001\n","activate function: ReLU\n","mini_batch_size: 32\n","Layer0:  784 neurons. \n","Layer1:  512 neurons. Weights:(512, 784), Bias:(512, 1)\n","Layer2:  256 neurons. Weights:(256, 512), Bias:(256, 1)\n","Layer3:   10 neurons. Weights:(10, 256), Bias:(10, 1)\n","Total number of params:535818\n"],"name":"stdout"}]},{"metadata":{"id":"luS45ilgalFI","colab_type":"code","outputId":"c44ea4a1-127e-4ed3-de70-5ed16d14e6ae","executionInfo":{"status":"ok","timestamp":1550182015900,"user_tz":300,"elapsed":333869,"user":{"displayName":"Qiang YE","photoUrl":"https://lh6.googleusercontent.com/-k-9mYNphfpc/AAAAAAAAAAI/AAAAAAAAARQ/zyID3sFntfM/s64/photo.jpg","userId":"15200210150486633975"}},"colab":{"base_uri":"https://localhost:8080/","height":881}},"cell_type":"code","source":["init_modes = [\"Zero\", \"Normal\", \"Glorot\"]\n","mode_losses = []\n","for init_mode in init_modes:\n","    print(\"weight init mode: {}\".format(init_mode))\n","    mlp.initialize_weights(hidden_dims = (512, 256), init_mode = init_mode)\n","    losses, accuracies, best_acc = mlp.train(batch_size = 128, \n","                                             learning_rate = 1e-2,\n","                                             max_epochs = 10)\n","    mode_losses.append(losses)\n","    print(\"\")"],"execution_count":13,"outputs":[{"output_type":"stream","text":["weight init mode: Zero\n","hidden dims:(512, 256) lr:0.01, Relu, batch size:128,\n","max epochs:10, early stopping:True, patience:20\n","[Epoch  0/9 100.00% 0m10s] train loss: 2.3021, accuracy: 11.33%  val loss: 2.3017, accuracy: 11.35%  *\n","[Epoch  1/9 100.00% 0m11s] train loss: 2.3015, accuracy: 11.36%  val loss: 2.3013, accuracy: 11.35% \n","[Epoch  2/9 100.00% 0m10s] train loss: 2.3013, accuracy: 11.36%  val loss: 2.3011, accuracy: 11.35% \n","[Epoch  3/9 100.00% 0m10s] train loss: 2.3011, accuracy: 11.36%  val loss: 2.3011, accuracy: 11.35% \n","[Epoch  4/9 100.00% 0m10s] train loss: 2.3011, accuracy: 11.36%  val loss: 2.3010, accuracy: 11.35% \n","[Epoch  5/9 100.00% 0m10s] train loss: 2.3011, accuracy: 11.36%  val loss: 2.3010, accuracy: 11.35% \n","[Epoch  6/9 100.00% 0m10s] train loss: 2.3011, accuracy: 11.36%  val loss: 2.3010, accuracy: 11.35% \n","[Epoch  7/9 100.00% 0m11s] train loss: 2.3010, accuracy: 11.36%  val loss: 2.3010, accuracy: 11.35% \n","[Epoch  8/9 100.00% 0m11s] train loss: 2.3010, accuracy: 11.36%  val loss: 2.3010, accuracy: 11.35% \n","[Epoch  9/9 100.00% 0m11s] train loss: 2.3010, accuracy: 11.36%  val loss: 2.3010, accuracy: 11.35% \n","Training complete in 1m 47s\n","Best val Acc on val:   11.35%\n","\n","weight init mode: Normal\n","hidden dims:(512, 256) lr:0.01, Relu, batch size:128,\n","max epochs:10, early stopping:True, patience:20\n","[Epoch  0/9 100.00% 0m14s] train loss: 28.9835, accuracy: 82.85%  val loss: 15.3436, accuracy: 88.76%  *\n","[Epoch  1/9 100.00% 0m12s] train loss: 11.9952, accuracy: 89.82%  val loss: 11.4538, accuracy: 89.60%  *\n","[Epoch  2/9 100.00% 0m12s] train loss: 8.0931, accuracy: 91.25%  val loss: 9.7020, accuracy: 90.30%  *\n","[Epoch  3/9 100.00% 0m12s] train loss: 5.9010, accuracy: 92.47%  val loss: 8.7389, accuracy: 90.42%  *\n","[Epoch  4/9 100.00% 0m12s] train loss: 4.5341, accuracy: 93.27%  val loss: 7.8555, accuracy: 90.84%  *\n","[Epoch  5/9 100.00% 0m12s] train loss: 3.5709, accuracy: 93.94%  val loss: 7.3948, accuracy: 90.86%  *\n","[Epoch  6/9 100.00% 0m11s] train loss: 2.9484, accuracy: 94.53%  val loss: 7.0081, accuracy: 91.14%  *\n","[Epoch  7/9 100.00% 0m11s] train loss: 2.4454, accuracy: 94.99%  val loss: 6.8382, accuracy: 91.10% \n","[Epoch  8/9 100.00% 0m11s] train loss: 2.0985, accuracy: 95.36%  val loss: 6.7344, accuracy: 91.02% \n","[Epoch  9/9 100.00% 0m11s] train loss: 1.7856, accuracy: 95.65%  val loss: 6.2423, accuracy: 91.55%  *\n","Training complete in 1m 58s\n","Best val Acc on val:   91.55%\n","\n","weight init mode: Glorot\n","hidden dims:(512, 256) lr:0.01, Relu, batch size:128,\n","max epochs:10, early stopping:True, patience:20\n","[Epoch  0/9 100.00% 0m11s] train loss: 1.2099, accuracy: 73.47%  val loss: 0.5759, accuracy: 86.84%  *\n","[Epoch  1/9 100.00% 0m11s] train loss: 0.4859, accuracy: 87.96%  val loss: 0.3891, accuracy: 90.01%  *\n","[Epoch  2/9 100.00% 0m10s] train loss: 0.3781, accuracy: 89.85%  val loss: 0.3288, accuracy: 91.20%  *\n","[Epoch  3/9 100.00% 0m10s] train loss: 0.3321, accuracy: 90.86%  val loss: 0.2975, accuracy: 91.93%  *\n","[Epoch  4/9 100.00% 0m10s] train loss: 0.3039, accuracy: 91.57%  val loss: 0.2779, accuracy: 92.38%  *\n","[Epoch  5/9 100.00% 0m11s] train loss: 0.2836, accuracy: 92.03%  val loss: 0.2633, accuracy: 92.83%  *\n","[Epoch  6/9 100.00% 0m11s] train loss: 0.2673, accuracy: 92.51%  val loss: 0.2477, accuracy: 93.24%  *\n","[Epoch  7/9 100.00% 0m11s] train loss: 0.2535, accuracy: 92.85%  val loss: 0.2394, accuracy: 93.30%  *\n","[Epoch  8/9 100.00% 0m11s] train loss: 0.2415, accuracy: 93.15%  val loss: 0.2274, accuracy: 93.67%  *\n","[Epoch  9/9 100.00% 0m11s] train loss: 0.2311, accuracy: 93.45%  val loss: 0.2184, accuracy: 93.85%  *\n","Training complete in 1m 48s\n","Best val Acc on val:   93.85%\n","\n"],"name":"stdout"}]},{"metadata":{"id":"5kE4XPDwjhGk","colab_type":"text"},"cell_type":"markdown","source":["#### 2. Ploting and comments training loss on 10 epochs using three different weight initializaton methods\n","\n","The curves are shown by executing codes in next cell. During the first 10 epochs with mini batch size = 32: \n","- Initialization with  **Zero** weights, training loss does not change, the model can learn nothing from the training dataset;\n","- Initialization with **Normal** weights, training loss decreases, while losses are almost ten times larger than losses using Zero and Glorot weight initialization methods. This could happen when **Cross Entropy** function is used to calculate loss, the probability output of true label can be much smaller than the average probability, 0.1 in this case. \n","- Initialization with **Glorot** weights, training loss decreases, with the initial probability output of true label very close to the average probability (0.1 in this cases)\n","\n","The **Glorot** weights initialization methods makes the weights more close to zero and uniformely distributed within a small range around,  which makes the output of the network more uniformely at the begining, while still keeping the ability to quickly learn from training dataset. Method **Zero** initializes all weights to zero such that all gradients are zero and the model cannot learn from training dataset. Method **Normal** assigns relatively large parameters that results in a relatively large scale of initial loss and the decreasing of loss during epochs. "]},{"metadata":{"id":"NJdYI7V3bFt9","colab_type":"code","outputId":"bcf8b558-f90c-43ac-c615-abf0de927d44","executionInfo":{"status":"ok","timestamp":1550182046741,"user_tz":300,"elapsed":1665,"user":{"displayName":"Qiang YE","photoUrl":"https://lh6.googleusercontent.com/-k-9mYNphfpc/AAAAAAAAAAI/AAAAAAAAARQ/zyID3sFntfM/s64/photo.jpg","userId":"15200210150486633975"}},"colab":{"base_uri":"https://localhost:8080/","height":512}},"cell_type":"code","source":["plt.figure(figsize = (10, 8))\n","epoch_data = np.arange(max_epochs)\n","for i in range(len(init_modes)):\n","    plt.plot(epoch_data, mode_losses[i][\"train\"], label = init_modes[i])\n","plt.xlabel('Epochs')\n","plt.ylabel('Losses')\n","plt.legend(loc='upper right')\n","plt.title(\"Loss on training dataset using different weight initialization style\")\n","plt.show()"],"execution_count":14,"outputs":[{"output_type":"display_data","data":{"image/png":"iVBORw0KGgoAAAANSUhEUgAAAlwAAAHvCAYAAACBqgH2AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAAAASUVORK5CYII=\n","text/plain":["<Figure size 720x576 with 1 Axes>"]},"metadata":{"tags":[]}}]},{"metadata":{"id":"ahLOGMEz5v2e","colab_type":"text"},"cell_type":"markdown","source":["### Hyperparameter Search\n","\n","Below cell helps to search best hyper-parameters settings for our neural model training on MNIST dataset. \n","So far, combinations of hyper-parameters we tried all achieved 97% accuracy on validation dataset.  We found that training progress on diferent values of `batch_size`and `learning_rate` differed a lot. Usually, certain batch_size will utilize the efficiency of matrix operation while decrease the speed of the convergence.\n"," \n"," Below are the accuracies on validation dataset of different combinations of hyper-parameters we tried. The model will terminate training once the average accuracy on validation dataset achieves 97%.\n","\n","\\begin{array} \n","&\\text{hidden_dims}  & \\text{act_function} & \\text{learning_rate}   &      \\text{batch_size}     & \\text{best train accuracy}     & \\text{best_val_accuracy}  & \\text{epoch of best val acc}\\\\\n","\\hline\n","   (512, 256) & \\text{Sigmoid} & 0.1  & 1 &    98.04\\% & 97.53\\%     &        3 \\\\\n","   (512,256) & \\text{Relu} & 0.0001  & 1 &    97.82\\% & 97.00\\%     &  37        \\\\\n","   (512,512) & \\text{Sigmoid} & 0.1  & 32 &    97.85\\% & 97.14\\%     & 24         \\\\\n","   (512,512) & \\text{Sigmoid} & 0.1  & 1 &    98.27\\% & 97.56\\%     & 4        \\\\\n","   (512,512) & \\text{Relu} & 0.01  & 32 &    97.78\\% &  97.09\\%     & 12        \\\\\n","      \\hline\\\\\n"," \\end{array}\n"," \n"]},{"metadata":{"id":"Iwlp8hTdVDYb","colab_type":"code","colab":{}},"cell_type":"code","source":["hidden_dimses = [(512, 256), (512, 512)]\n","learning_rates = [1e-1, 1e-2, 1e-4]\n","activate_funcs = [\"Relu\", \"Sigmoid\"]\n","mini_batch_sizes = [1, 32, 256]\n","settings_trained = [(0,0,0,0), (0,0,1,0), (1,0,1,1), (1,1,0,0), (1,1,1,0)]\n","\n","\n","n_dims = len(hidden_dimses)\n","n_lrs =  len(learning_rates)\n","n_funcs = len(activate_funcs)\n","n_batch_size = len(mini_batch_sizes)\n","\n","n_search = n_dims * n_lrs * n_funcs * n_batch_size\n","\n","mlp = NN(data_path = 'mnist.pkl.gz')\n","\n","for i in range(n_search):\n","    print(\"-\"* 30)\n","    dims_index, lr_index, func_index = 0, 0, 0\n","    while True:\n","        dims_index = np.random.randint(n_dims)\n","        lr_index = np.random.randint(n_lrs)\n","        func_index = np.random.randint(n_funcs)\n","        batch_size_index = np.random.randint(n_batch_size)\n","        \n","        setting = (dims_index, lr_index, func_index, batch_size_index)\n","        if not setting in settings_trained:\n","            settings_trained.append(setting)\n","            break\n","    \n","    hidden_dims = hidden_dimses[dims_index]\n","    learning_rate = learning_rates[lr_index]\n","    activate_func = activate_funcs[func_index]\n","    batch_size = mini_batch_sizes[batch_size_index]\n","        \n","    mlp.initialize_weights(hidden_dims = hidden_dims, init_mode = \"Glorot\")\n","    losses, accuracies, best_acc = mlp.train(batch_size = batch_size, \n","                                             learning_rate = learning_rate,\n","                                             activate_func = activate_func,\n","                                             max_epochs = 50,\n","                                             epoch_patience = 5,\n","                                             show_live_progress = True)\n"],"execution_count":0,"outputs":[]},{"metadata":{"id":"QO9j9Ntw5mSY","colab_type":"text"},"cell_type":"markdown","source":["### Validate Gradients using Finite Difference\n","\n","Two following functions are implemented to perform the gradient check using **one** training sample. Gradient  check of bias is also supported. See comments of the codes for more details of the implementation.  "]},{"metadata":{"id":"1c8iEcWWLFNj","colab_type":"code","colab":{}},"cell_type":"code","source":["def grad_finite_diff(mlp, x, label, \n","                     layer_index = 1, \n","                     param_index = 0,\n","                     epsilon = 1e-5,\n","                     use_bias = False):\n","    \"\"\"compute gradient of a specified parameter using finite different method.\n","    on one training sample\n","    params\n","        x: a sample (1, 784)\n","        y: label, int\n","        layer_index: 1 means the first hidden layer\n","        param_index: 0 means the first wieght parameter in that layer\n","        use_bias: whether params_index is of bias matrix\n","        epsilon: a delta interval of parameter\n","    returns\n","        finite different gradient of the parameter, float\n","    \"\"\"\n","    # choose a param (weights or bias tensor of a certain layer\n","    param = mlp.Ws[layer_index] if not use_bias else mlp.bs[layer_index]\n","    # access of the index of the param\n","    h_layer, h_layer_prev = param.shape\n","    if not use_bias:\n","        i = param_index // h_layer_prev\n","        j = param_index % h_layer_prev\n","    else: # check gradient of a bias\n","        i = param_index\n","        j = 0\n","    # access of old value of a single scalar parameter \n","    old_value = param[i, j]    \n","    # prepare one training sample\n","    x = x.reshape(1, -1)\n","    label = np.array([label])\n","    \n","    # calculate loss of (old_value + epsilon)\n","    value_plus = old_value + epsilon\n","    param[i, j] = value_plus\n","    cache = mlp.forward(x)\n","    loss_plus = mlp.loss(cache[\"ho\"], label)\n","    #print(\"param_plus:{}, loss_plus: {}\".format(value_plus, loss_plus))\n","    \n","    # calculate loss of (old_value - epsilon)\n","    value_minor = old_value - epsilon\n","    param[i ,j] = value_minor\n","    cache = mlp.forward(x)\n","    loss_minor = mlp.loss(cache[\"ho\"], label)\n","    #print(\"param_minor:{}, loss_mino: {}\".format(value_minor, loss_minor))\n","    \n","    # compute gradient by finite difference method.\n","    grad_finite = (loss_plus - loss_minor)/(2.0 * epsilon)\n","    param[i, j] = old_value # restore old value.\n","\n","    #cache = mlp.forward(x)\n","    #grad_back = mlp.backward(cache, label)[0][layer_index][i, j]\n","    \n","    #print(grad_finite, grad_back)\n","    return grad_finite #, grad_back \n","\n","\n","def validate_gradient(mlp, num_params = 10, layer_index = 0, use_bias = False,\n","                     verbose = True):\n","    \"\"\"perform gradient check \n","    Params\n","        mlp: neural network object, NN\n","        num_params: number of parameters to be check, int\n","        layer_index: index where the parameters are located, int\n","        use_bias: if check bias gradient, bool\n","        verbose: display more information, bool\n","    \"\"\"\n","    X, labels = mlp.load_data(dataset_type = \"train\")\n","    sample_size = X.shape[0]\n","    # randomly choose a training sample\n","    sample_index = np.random.randint(0, sample_size)\n","    x = X[sample_index,:].reshape(1, -1)\n","    label = np.array([labels[sample_index]])\n","\n","    # ensure layer_index is valid\n","    if layer_index < 1 or layer_index > mlp.n_hidden + 1:\n","        layer_index = 1\n","    # locate the param tensor for gradient check\n","    param = mlp.Ws[layer_index] if not use_bias else mlp.bs[layer_index]\n","    param_type = 0 if not use_bias else 1\n","    param_names = [\"WEIGHTS\", \"BIAS\"]\n","    \n","    m = param.shape[0] * param.shape[1]\n","    # first p elements in parameter matrix\n","    p = m if num_params is None else min(num_params, m) \n","    \n","    # gradient using backward method.\n","    cache = mlp.forward(x)\n","    \n","    grads_back = mlp.backward(cache, label)[param_type][layer_index]\n","    # use first p gradient values\n","    grads_back_p = grads_back.reshape(-1, )[0:p]\n","    \n","    if verbose:\n","        predict_label = np.argmax(cache[\"ho\"], axis = 1)\n","        loss = mlp.loss(cache[\"ho\"], label)\n","        print(\"validating gradient of {} of layer {}\".format(\n","            param_names[param_type], layer_index))\n","        print(\"num of total params:{}; p:{}\".format(m, p))    \n","        print(\"Sample: x.shape:{}, label:{}, predict_label:{}, loss:{}\"\n","              .format(x.shape, label, predict_label, loss))\n","        #print(\"first p={} backward grads:{}\".format(p, grads_back_p))\n","        print(\"{} non-zero of all {} backward grads\".format(\n","            np.count_nonzero(grads_back), m))\n","        print(\"{} non-zero of first {} backward grads\".format(\n","            np.count_nonzero(grads_back_p), p))\n","    #print(\"first p={} finite grads:\".format(p))\n","    max_diffs = []  # keep max difference for different epsilon/N\n","    # for different epsilon/N\n","    Ns = [k*np.power(10, i) for i in range(0, 6) for k in [1, 5] ]\n","    for N in Ns:\n","        epsilon = 1.0 / N\n","        grads_finite = np.zeros((p, ))\n","        for param_index in range(p):\n","            grads_finite[param_index] = grad_finite_diff(mlp, x, label, \n","                                                         layer_index, \n","                                                         param_index, \n","                                                         epsilon,\n","                                                         use_bias)\n","        #print(\"grads_finite:{}\".format(grads_finite))\n","        max_diff = np.max(np.abs(grads_finite - grads_back_p))\n","        max_diffs.append(max_diff)\n","        if verbose:\n","            print(\"N:{:<6}, epsilon:{:<6}, Grads All Close:{}, max_d_grads:{}\".\\\n","                  format(N, epsilon, np.allclose(grads_finite, grads_back_p), \n","                         max_diff))\n","    cache = {\"Ns\": Ns, \"max_diffs\": max_diffs, \"p\": p, \n","             \"layer_index\": layer_index, \"use_bias\": use_bias}            \n","    return cache"],"execution_count":0,"outputs":[]},{"metadata":{"id":"lLf2bCOv6DCh","colab_type":"text"},"cell_type":"markdown","source":["The architecture of the neural network used for gradient check is the output of the below code cell."]},{"metadata":{"id":"X0YBuoBU5bGh","colab_type":"code","outputId":"3618f398-b536-48b6-e33a-0bc1916b01dc","executionInfo":{"status":"ok","timestamp":1550181001375,"user_tz":300,"elapsed":633,"user":{"displayName":"Qiang YE","photoUrl":"https://lh6.googleusercontent.com/-k-9mYNphfpc/AAAAAAAAAAI/AAAAAAAAARQ/zyID3sFntfM/s64/photo.jpg","userId":"15200210150486633975"}},"colab":{"base_uri":"https://localhost:8080/","height":215}},"cell_type":"code","source":["mlp = NN(data_path = 'mnist.pkl.gz', activate_func = \"Relu\")\n","mlp.initialize_weights(hidden_dims = (512, 256), \n","                       init_mode = 'Glorot', \n","                       random_seed = 10)\n","mlp.net_info()"],"execution_count":24,"outputs":[{"output_type":"stream","text":["Activation function: Relu\n","2 hidden layers\n","Mode of weghts initialization: Glorot\n","learning rate: 0.0001\n","activate function: Relu\n","mini_batch_size: 1\n","Layer0:  784 neurons. \n","Layer1:  512 neurons. Weights:(512, 784), Bias:(512, 1)\n","Layer2:  256 neurons. Weights:(256, 512), Bias:(256, 1)\n","Layer3:   10 neurons. Weights:(10, 256), Bias:(10, 1)\n","Total number of params:535818\n"],"name":"stdout"}]},{"metadata":{"id":"qwXAB5f16Ks6","colab_type":"text"},"cell_type":"markdown","source":["The output of next code shows the max difference between two gradients from first p parameters of different N."]},{"metadata":{"id":"WGfNqZtvhLah","colab_type":"code","outputId":"042f9f10-1dab-40d7-97bf-58ee1c0710c6","executionInfo":{"status":"ok","timestamp":1550181021673,"user_tz":300,"elapsed":7464,"user":{"displayName":"Qiang YE","photoUrl":"https://lh6.googleusercontent.com/-k-9mYNphfpc/AAAAAAAAAAI/AAAAAAAAARQ/zyID3sFntfM/s64/photo.jpg","userId":"15200210150486633975"}},"colab":{"base_uri":"https://localhost:8080/","height":323}},"cell_type":"code","source":["cache = validate_gradient(mlp, num_params = 1000, layer_index = 2, \n","                  use_bias = False, \n","                  verbose = True)"],"execution_count":26,"outputs":[{"output_type":"stream","text":["validating gradient of WEIGHTS of layer 2\n","num of total params:131072; p:1000\n","Sample: x.shape:(1, 784), label:[8], predict_label:[4], loss:2.838183614164247\n","33750 non-zero of all 131072 backward grads\n","493 non-zero of first 1000 backward grads\n","N:1     , epsilon:1.0   , Grads All Close:False, max_d_grads:0.050374250712171637\n","N:5     , epsilon:0.2   , Grads All Close:False, max_d_grads:0.03171130070863891\n","N:10    , epsilon:0.1   , Grads All Close:False, max_d_grads:0.027701843271789026\n","N:50    , epsilon:0.02  , Grads All Close:True, max_d_grads:1.4545783480479635e-08\n","N:100   , epsilon:0.01  , Grads All Close:True, max_d_grads:3.636443382526444e-09\n","N:500   , epsilon:0.002 , Grads All Close:True, max_d_grads:1.4545810389510194e-10\n","N:1000  , epsilon:0.001 , Grads All Close:True, max_d_grads:3.6545225179374086e-11\n","N:5000  , epsilon:0.0002, Grads All Close:True, max_d_grads:1.7757427472897547e-12\n","N:10000 , epsilon:0.0001, Grads All Close:True, max_d_grads:3.0248442639546624e-12\n","N:50000 , epsilon:2e-05 , Grads All Close:True, max_d_grads:1.6712901895754584e-11\n","N:100000, epsilon:1e-05 , Grads All Close:True, max_d_grads:3.7237671279832796e-11\n","N:500000, epsilon:2e-06 , Grads All Close:True, max_d_grads:1.72866157152618e-10\n"],"name":"stdout"}]},{"metadata":{"id":"tLnXs-mP6ZeA","colab_type":"text"},"cell_type":"markdown","source":["The curve of max difference of the gradients of N is shown as following. As N increases, the $\\epsilon$ decreases which leads to a smaller difference between gradients calculated by method `backward` and thoses by finite different method. For this case, when $N \\ge 10$, all the gradients monitored are very close, which indicates our `backward` methods may be correctly implemented. Furthermore, for each parameter in the neural network, its gradient can be automatically checked by the codes. We performed the check of most parameters, including first 1000 weights and all bias of all layers. The results indicate that our `backward` function is correctly implemented."]},{"metadata":{"id":"kUDzAhUZ0VBW","colab_type":"code","outputId":"250ba75c-26e3-4a6e-a38a-73a12935d754","executionInfo":{"status":"ok","timestamp":1550181067958,"user_tz":300,"elapsed":1982,"user":{"displayName":"Qiang YE","photoUrl":"https://lh6.googleusercontent.com/-k-9mYNphfpc/AAAAAAAAAAI/AAAAAAAAARQ/zyID3sFntfM/s64/photo.jpg","userId":"15200210150486633975"}},"colab":{"base_uri":"https://localhost:8080/","height":534}},"cell_type":"code","source":["plt.figure(figsize = (10, 8))\n","plt.plot(cache[\"Ns\"], cache[\"max_diffs\"])\n","plt.xlabel('Ns')\n","plt.ylabel('max_d_grads')\n","plt.xscale('log')\n","#plt.yscale('log')\n","param_type = \"Bias\" if cache[\"use_bias\"] else \"Weights\"\n","plt.title(\"Max difference of {} gradients of layer {} (p = {})\".format(\n","     param_type, cache[\"layer_index\"], cache[\"p\"]))\n","#plt.show()"],"execution_count":29,"outputs":[{"output_type":"execute_result","data":{"text/plain":["Text(0.5, 1.0, 'Max difference of Weights gradients of layer 2 (p = 1000)')"]},"metadata":{"tags":[]},"execution_count":29},{"output_type":"display_data","data":{"image/png":"iVBORw0KGgoAAAANSUhEUgAAAmUAAAHzCAYAAABhWGrkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n","text/plain":["<Figure size 720x576 with 1 Axes>"]},"metadata":{"tags":[]}}]},{"metadata":{"id":"Z7a-rYLo8QgK","colab_type":"text"},"cell_type":"markdown","source":["The max difference increases slightly when $\\lg N > 4$ may be the result of the limitation of the representation accuracy from a division operation on extremely small float numbers."]}]}