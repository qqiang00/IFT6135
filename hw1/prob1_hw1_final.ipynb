{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"prob1_hw1_final.ipynb","version":"0.3.2","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"metadata":{"id":"yoj3v7b5Nciw","colab_type":"text"},"cell_type":"markdown","source":["# <center>Report of practical part of Assignment 1</center>\n","# <center>IFT6135-H19</center>\n","\n","This file contains the report of problem1,2, and 3 of practical part of assignment1.\n","\n","**Codes links**:\n","\n","problem1: https://github.com/qqiang00/IFT6135/blob/master/hw1/prob1_hw1_final.ipynb\n","\n","problem2: https://github.com/qqiang00/IFT6135/blob/master/hw1/prob2_hw1_final.ipynb\n","\n","problem3: https://github.com/qqiang00/IFT6135/blob/master/hw1/prob3_hw1_final.ipynb\n","\n","**Team member**: \n","- Qiang Ye (20139927)\n","- Ying Xiao (20111402)\n","- Boumaza Amel (20126028)\n","- Yunhe Li (20137167)"]},{"metadata":{"id":"X3c-h5FjOQEw","colab_type":"text"},"cell_type":"markdown","source":["## Support for Computation on Google Colab\n","\n","Ignore this part if the codes are not being executed on Google Colab."]},{"metadata":{"id":"IvGFIzH4_wnQ","colab_type":"code","outputId":"e1047acf-8d75-4ab5-f400-caa756654325","executionInfo":{"status":"ok","timestamp":1550358444874,"user_tz":300,"elapsed":22058,"user":{"displayName":"Qiang YE","photoUrl":"https://lh6.googleusercontent.com/-k-9mYNphfpc/AAAAAAAAAAI/AAAAAAAAARQ/zyID3sFntfM/s64/photo.jpg","userId":"15200210150486633975"}},"colab":{"base_uri":"https://localhost:8080/","height":35}},"cell_type":"code","source":["from os.path import exists\n","from google.colab import drive\n","import os\n","drive.mount('/content/dirve/', force_remount = True)"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Mounted at /content/dirve/\n"],"name":"stdout"}]},{"metadata":{"id":"upqtlSNSA7jP","colab_type":"code","colab":{}},"cell_type":"code","source":["# change to your own directory\n","os.chdir(\"dirve/My Drive/Colab Notebooks/IFT6135/hw1/\")\n","#!ls -al"],"execution_count":0,"outputs":[]},{"metadata":{"id":"-trqNYyA0opF","colab_type":"text"},"cell_type":"markdown","source":["## Problem 1\n","\n"]},{"metadata":{"id":"dKaPEhSh7lYl","colab_type":"text"},"cell_type":"markdown","source":["**Notes**:\n","Before running the codes in problem1, make sure you have the following files in your current working directory\n","- `utils.py`: contains two tool functions (`debug` and `print_progress`)\n","- `mnist.pkl.gz`: MNIST dataset. \n","   \n","   Both files can be downloaded here:  https://github.com/qqiang00/IFT6135/tree/master/hw1"]},{"metadata":{"id":"X8OSRrlzWCeD","colab_type":"text"},"cell_type":"markdown","source":["### 1. Building the Model\n","\n"]},{"metadata":{"id":"SPjKw_FxWD5v","colab_type":"text"},"cell_type":"markdown","source":["#### 1). Possible hidden dims settings for number of parameters fallen into [0.5M, 1M]\n","\n","Consider a MLP with 2 hidden layers with $h^{(1)}$ and $h^{(2)}$ hidden units, an input layer with feature number of $h^{(0)} = 784$ and an output layer with number of labels $h^{(3)} = 10$,  the total number of parameters($\\#P\\_total$) is:\n","\n","\\begin{align*}\n","\\#P\\_total &= \\#P\\_hidden1 + \\#P\\_hidden2 + \\#P\\_output\\\\\n","&= h^{(1)}  (h^{(0)} + 1) + h^{(2)}(h^{(1)} + 1) + h^{(3)}(h^{(2)}+1)\\\\\n","\\end{align*}\n","\n","The output of the next two code cells tells possible hidden dimensions are:\n","\n","(128, 4096), (256, 2048), (512, 256), (512, 512), (512, 1024), (1024, 128)\n","\n","For the following questions, (512,256) is our preferred parameter setting for hidden dimensions."]},{"metadata":{"id":"AHItzlBZ3R7N","colab_type":"code","colab":{}},"cell_type":"code","source":["def num_parameters(h1, h2, h0 = 784, h3 = 10):\n","    p_hidden1 = h1 * (h0 + 1)\n","    p_hidden2 = h2 * (h1 + 1)\n","    p_output = h3 * (h2 + 1)\n","    p_total = p_hidden1 + p_hidden2 + p_output\n","    return p_total"],"execution_count":0,"outputs":[]},{"metadata":{"id":"Y_kMGKdr3roG","colab_type":"code","outputId":"940e9356-8ab5-46c4-9786-a0bffa57188f","executionInfo":{"status":"ok","timestamp":1550358453094,"user_tz":300,"elapsed":259,"user":{"displayName":"Qiang YE","photoUrl":"https://lh6.googleusercontent.com/-k-9mYNphfpc/AAAAAAAAAAI/AAAAAAAAARQ/zyID3sFntfM/s64/photo.jpg","userId":"15200210150486633975"}},"colab":{"base_uri":"https://localhost:8080/","height":143}},"cell_type":"code","source":["dim_options = [128, 256, 512, 1024, 2048, 4096]\n","print(\"Possible values of (h1, h2):\")\n","\n","for h1 in dim_options:\n","    for h2 in dim_options:\n","        if 5e5 <= num_parameters(h1, h2) <= 1e6:\n","            print(\"({}, {})\".format(h1, h2))"],"execution_count":4,"outputs":[{"output_type":"stream","text":["Possible values of (h1, h2):\n","(128, 4096)\n","(256, 2048)\n","(512, 256)\n","(512, 512)\n","(512, 1024)\n","(1024, 128)\n"],"name":"stdout"}]},{"metadata":{"id":"VWKYy5dV55Jd","colab_type":"text"},"cell_type":"markdown","source":["#### 2). Implementation of a MLP class\n","\n","In this section, the most difficult parts are to implement `forward`, `backward`, and `train` methods. \n","\n","Our implementation of this MLP class has the following features:\n","- support more than 2 hidden layers;\n","- self-defined input and output dimensions;\n","- softmax as final classifier;\n","- 3 options of weights initialization methods: \"zero\", \"normal\", and \"glorot\";\n","- 3 options of activation function for outputs of hidden layers: \"sigmoid\", \"relu\", \"tanh\";\n","- matrix operation during forward and backward propagation with batch_size = 1 supported.\n","- support early stopping;\n","- save and load model from a file;\n","- lively display expected training time for each epoch;\n","- show network architecture and hyper-parameter settings.\n","\n","Code details are the following:"]},{"metadata":{"id":"_Skm84RoxQku","colab_type":"code","colab":{}},"cell_type":"code","source":["%matplotlib inline\n","from utils import print_progress # print training progress\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import pickle\n","import gzip\n","import time"],"execution_count":0,"outputs":[]},{"metadata":{"id":"FRzRvRTWxY-V","colab_type":"code","colab":{}},"cell_type":"code","source":["class NN(object):\n","    def __init__(self, \n","                 #hidden_dims = (512, 256), \n","                 #n_hidden = None, \n","                 learning_rate = 1e-4,\n","                 max_epochs = 10,\n","                 batch_size = 1, # stochastic gradient descent\n","                 input_dim = 784,\n","                 output_dim = 10,\n","                 activate_func = \"ReLU\",\n","                 early_stop = False,\n","                 mode = 'train', \n","                 #weight_init_mode = 'Glorot',\n","                 data_path = None, \n","                 model_path = None,\n","                 verbose = False,\n","                 #random_seed = 0\n","                ):\n","        self.input_dim = input_dim\n","        self.output_dim = output_dim\n","        self.hidden_dims = None # hidden_dims\n","        self.dims = None # self._generate_dims()\n","        self.n_hidden = None\n","        #if type(hidden_dims) in [list, tuple]:\n","        #    self.n_hidden = len(hidden_dims)\n","        #else:\n","        #    self.n_hidden = n_hidden # number of hidden layers\n","        \n","        # Ws[0],bs[0] always be None\n","        # Ws[1] connects inputlayer to 1st hidden layer\n","        # bs[1] is the bias of 1st hidden layer\n","        self.Ws = None #[None] * (self.n_hidden + 2) \n","        self.bs = None #[None] * (self.n_hidden + 2) \n","\n","        self.activate_func = activate_func # nonlinear activate function\n","        self.lr = learning_rate\n","        self.max_epochs = max_epochs\n","        self.batch_size = batch_size\n","        self.early_stop = early_stop\n","        self.verbose = verbose\n","        self.mode = mode\n","        self.data_path = data_path\n","        self.model_path = model_path\n","        self.random_seed = None # random_seed\n","        \n","        self.weight_init_mode = None # weight_init_mode\n","        return\n","    \n","    \n","    def _generate_dims(self):\n","        \"\"\"generate the nn layer dims, including the input and output layer\n","        \"\"\"\n","        dims = [self.input_dim]\n","        dims.extend(list(self.hidden_dims))\n","        dims.extend([self.output_dim])\n","        return dims\n","    \n","        \n","    def load_data(self, dataset_type = \"train\"):\n","        \"\"\"load specified dataset from data_path\n","        Params\n","            dataset_type: specify the type of the dataset, str. Example: \"train\"\n","            ,\"valid\", \"test\"\n","        Returns\n","            dataset, tuple (X, y) where shape of X is (sample_size, n_feature), \n","            shape of y is (sample_size, )\n","        \"\"\"\n","        with gzip.open(self.data_path,'rb') as f:\n","            u = pickle._Unpickler(f)\n","            u.encoding = 'latin1'\n","            data_set = u.load()\n","            # all _data are tuple with 2 elements: X, and y with shape like\n","            # (50000, 784) and (50000, ) respectively\n","        dataset_type = dataset_type.upper()\n","        if dataset_type == \"TRAIN\":\n","            return data_set[0]\n","        elif dataset_type == \"VALID\":\n","            return data_set[1]\n","        else: # test\n","            return data_set[2]\n","    \n","        \n","    def initialize_weights(self, hidden_dims, n_hidden = None,\n","                           init_mode = 'Glorot', random_seed = 0):\n","        \"\"\"initialize weights and bias according to the attributes given.\n","        \"\"\"\n","        self.hidden_dims = hidden_dims\n","        if type(hidden_dims) in [list, tuple]:\n","            self.n_hidden = len(hidden_dims)\n","        else:\n","            self.n_hidden = n_hidden # number of hidden layers\n","        \n","        self.dims = self._generate_dims()\n","        self.random_seed = random_seed\n","        self.weight_init_mode = init_mode\n","        \n","        self.Ws = [None] * (self.n_hidden + 2)\n","        self.bs = [None] * (self.n_hidden + 2)\n","        \n","        np.random.seed(self.random_seed)\n","        dims = self.dims\n","        \n","        for i in range(1, len(dims)): \n","            init_mode = init_mode.upper()\n","            weight_shape = (dims[i], dims[i-1])\n","            if init_mode == \"ZERO\":\n","                self.Ws[i] = np.zeros(weight_shape)\n","            elif init_mode == \"NORMAL\":\n","                self.Ws[i] = np.random.normal(0, 1, weight_shape)\n","            else: # â€˜Glorot'\n","                high = float(np.sqrt(6.0/(dims[i] + dims[i-1])))\n","                self.Ws[i] = np.random.uniform(-1 * high, high, weight_shape)\n","\n","            self.bs[i] = np.zeros((dims[i], 1))\n","                    \n","        return\n","    \n","    \n","    def zero_grad(self):\n","        \"\"\"prepare zero grad of weight and bias parameters\n","        Params\n","            None\n","        Returns\n","            grad_Ws, grad_bs\n","        \"\"\"\n","        grad_Ws = [None] * (self.n_hidden + 2) # grad_Ws[0] always None\n","        grad_bs = [None] * (self.n_hidden + 2) # grad_bs[0] always None\n","        for i in range(1, self.n_hidden+2):\n","            grad_Ws[i] = np.zeros_like(self.Ws[i])\n","            grad_bs[i] = np.zeros_like(self.bs[i])\n","        return grad_Ws, grad_bs\n","    \n","    \n","    def forward(self, X):\n","        \"\"\"forward process of the network\n","        Params\n","            X: input (batch_size, input_features)\n","        Returns\n","            cache: a dictionary of all intermediate and final output.\n","        \"\"\"\n","        h_x = X # h_x is output of the layer\n","        cache = {\"X\": X, \"h0\": X} # there's no \"a0\"\n","        for i in range(1, self.n_hidden + 2):\n","            # a_x is pre-activate value of a layer\n","            # h_x is output of a layer\n","            a_x = np.dot(h_x, self.Ws[i].T) + self.bs[i].T\n","            if i == self.n_hidden + 1: # output layer\n","                h_x = self.softmax(a_x)\n","                cache[\"ao\"] = a_x\n","                cache[\"ho\"] = h_x\n","            else: # hidden layer(s)\n","                h_x = self.activation(a_x, self.activate_func)\n","                cache[\"a\"+str(i)] = a_x\n","                cache[\"h\"+str(i)] = h_x\n","        \n","        return cache\n","    \n","    \n","    def activation(self, X, func = \"RELU\"):\n","        \"\"\"compute activation of a Tensor\n","        Params\n","            X \n","        \"\"\"\n","        func = func.upper() #self.activate_func.upper()\n","        if func == \"TANH\":\n","            e_pos = np.exp(X)\n","            e_neg = np.exp(-1 * X)\n","            return (e_pos - e_neg)/(e_pos + e_neg)\n","        elif func == \"SIGMOID\":\n","            return 1.0/(1 + np.exp(-1 * X))\n","        else:# func in ['ReLU', 'relu', 'RELU', 'Relu']:\n","            return np.maximum(X, 0)\n","        \n","        \n","    def _derivative(self, h, func = \"RELU\"):\n","        \"\"\" derivative of certain output of activation functions using \n","        output of activation function\n","        \"\"\"\n","        func = func.upper() #self.activate_func.upper()\n","        if func == 'TANH':\n","            return 1 - np.power(h, 2)\n","        elif func == 'SIGMOID':\n","            return np.multiply(h, 1-h)\n","        else:# func in ['ReLU', 'relu', 'RELU', 'Relu']:\n","            return np.sign(h)\n","        \n","    \n","    def loss(self, ho, labels):\n","        \"\"\"compute average loss\n","        params\n","            ho: output of network (batch_size, output_dim)\n","            labels: true label (batch_size, class_index)\n","        returns\n","            loss, average loss on batch_size samples, float\n","        \"\"\"\n","        #print(prediction)\n","        batch_size, output_dim = ho.shape\n","        #labels = labels.reshape(-1, 1)\n","        labels = labels.astype(int)\n","\n","        ho_labels_probs = ho[np.arange(batch_size), labels]\n","        # to avoid log(0)\n","        ho_labels_probs = np.maximum(ho_labels_probs, 1e-100)\n","        loss = np.mean(-np.log(ho_labels_probs))\n","        return float(loss)\n","    \n","    \n","    def accuracy(self, ho, labels):\n","        \"\"\"compute average accuracy\n","        params\n","            ho: output of network (batch_size, output_dim)\n","            labels: true label (batch_size, class_index)\n","        returns\n","            accuracy, average accuracy on batch_size samples, float\n","        \"\"\"\n","        batch_size, output_dim = ho.shape\n","        labels = labels.reshape(-1, )\n","        labels = labels.astype(int)\n","        ho_labels = np.argmax(ho, axis = 1)\n","        return float(np.sum(ho_labels == labels) / len(ho_labels))\n","\n","    \n","    def softmax(self, ao):\n","        \"\"\"return softmax of pre-activate value of final layer\n","        Params\n","            ao: pre-activate value of final layer, (batch_size, n_class)\n","        Returns\n","            probs: (batch_size, n_class)\n","        \"\"\"\n","        if len(ao.shape) == 1: # shape like (4,)\n","            ao = ao.reshape(1, -1)\n","        \n","        ao_max = np.max(ao, axis = 1, keepdims = True)\n","        sum_exp_ao = np.sum(np.exp(ao - ao_max), axis = 1, keepdims = True)\n","        return np.exp(ao - ao_max) / sum_exp_ao\n","    \n","    \n","    def _one_hot(self, output_dim, labels):\n","        labels = labels.reshape(-1, ) # (batch_size, )\n","        labels = labels.astype(int)\n","        batch_size = labels.shape[0]\n","        result = np.zeros((batch_size, output_dim))\n","        result[np.arange(batch_size), labels] = 1\n","        return result\n","    \n","    \n","    def backward(self, cache, labels):\n","        \"\"\"backward process of a neural network\n","        Params\n","            cache: output of function forward, a dictionary of intermediate \n","                output neural network\n","            labels:true label of dataset\n","        Returns\n","            (grads_Ws, grads_bs), tuple \n","        \"\"\"\n","        ao = cache[\"ao\"]\n","        ho = cache[\"ho\"]\n","        X = cache[\"X\"]\n","        grad_Ws, grad_bs = self.zero_grad()\n","        \n","        batch_size, output_dim = ho.shape\n","        # skip calculating grad_ho, we directly calculate grad_ao,\n","        # as grad_ao is easy to be calculated.\n","        grad_ao = ho - self._one_hot(output_dim, labels)\n","        \n","        i = self.n_hidden + 1 #  i = 3 if n_hidden = 2\n","        grad_Ws[i] = np.dot(grad_ao.T, cache[\"h\"+str(i-1)])\n","        grad_bs[i] = grad_ao\n","        \n","        grad_a = grad_ao\n","        for i in range(self.n_hidden, 0, -1):\n","            # a is pre-activate value of a (hidden) layer\n","            # h is a output of a layer.\n","            a = cache[\"a\"+str(i)] # pre-activation\n","            h = cache[\"h\"+str(i)] # activated output\n","            \n","            grad_h = np.dot(grad_a, self.Ws[i+1])\n","            grad_a = np.multiply(grad_h, self._derivative(h, self.activate_func))\n","            \n","            grad_Ws[i] = np.dot(grad_a.T, cache[\"h\"+str(i-1)])\n","            grad_bs[i] = grad_a\n","            \n","       \n","        for i in range(self.n_hidden+1, 0, -1):\n","            grad_Ws[i] /= batch_size\n","            grad_bs[i] = np.mean(grad_bs[i], axis = 0, keepdims = True).T\n","\n","        grads = (grad_Ws, grad_bs)\n","        return grads\n","    \n","    \n","    def update(self, grads, learning_rate = 1e-4):\n","        \"\"\"pudate parameters\n","        Params\n","            learning_rate: learning_rate, float\n","        Returns\n","            None\n","        \"\"\"\n","        lr = learning_rate\n","        for i in range(1, self.n_hidden + 2):\n","            self.Ws[i] -= lr * grads[0][i]\n","            self.bs[i] -= lr * grads[1][i]\n","        \n","        return\n","    \n","            \n","    def train(self, \n","              batch_size = 1,  \n","              learning_rate = 1e-3, \n","              activate_func = \"Relu\",\n","              show_live_progress = True,\n","              epoch_patience = 20,\n","              max_epochs = 10,\n","              early_stop = True,\n","              model_path = 'model_auto_save.pkl',\n","              auto_save_interval = 10\n","             ):\n","        \"\"\"training a neural network. at each epoch, training the model on \n","        training dataset, and then compute loss and accuracy on validate dataset\n","        Params\n","            batch_size: mini_batch_size, int\n","            learning_rate: learning_rate, float\n","            activate_func: activate function, str\n","            show_live_progress: if display live training progress, bool\n","            epoch_patience: patient epoch for early stopping, int\n","            max_epochs: max epochs for training, int\n","            early_stop: if early stopping is applicable, bool\n","        Returns\n","            (losses, accuracies, best_acc)\n","            losses: {\"train\":[], \"val\":[]} \n","            accuracies: {\"train\":[], \"val\":[]}\n","            best_acc: best accuracy on validate set.\n","        \"\"\"\n","        \n","        self.activate_func = activate_func\n","        #self.mode = mode\n","        self.max_epochs = max_epochs\n","        self.batch_size = batch_size\n","        self.lr = learning_rate\n","        self.early_stop = early_stop\n","        \n","        #print(\"traing hyper-parameters:\")\n","        print(\"hidden dims:{}\".format(self.hidden_dims), end = \" \")\n","        print(\"lr:{},\".format(self.lr), end = \" \")\n","        print(\"{},\".format(self.activate_func), end = \" \")\n","        print(\"batch size:{},\".format(self.batch_size))\n","        print(\"max epochs:{},\".format(self.max_epochs), end = \" \")\n","        print(\"early stopping:{},\".format(early_stop), end = \" \")\n","        print(\"patience:{}\".format(epoch_patience))\n","        \n","        best_acc = {'train':0.0, 'val':0.0} \n","        losses = {'train': [], 'val':[]}\n","        accuracies = {'train': [], 'val':[]}\n","        patience_used = 0         # patience used\n","        early_stopped = 0\n","        start = time.time()\n","        dataset = {\"train\": self.load_data(\"train\"),\n","                  \"val\": self.load_data(\"val\")}\n","        \n","        for epoch in range(self.max_epochs):\n","            since  = time.time()\n","            s_before = '[Epoch{:>3d}/{} '.format(epoch, self.max_epochs - 1)\n","            s_after = ']'\n","            if show_live_progress:\n","                print_progress(0, 0, s_before, s_after)\n","            else:\n","                # print(s_before + s_after, end = \"\")\n","                pass\n","            \n","            for mode in ['train', 'val']:\n","                since_mode = time.time()\n","                \n","                loss, acc = 0.0, 0.0\n","                if show_live_progress:\n","                    print_progress(0, 0, s_before, s_after)\n","                \n","                X, y = dataset[mode]\n","                # shuffle data during training \n","                if mode == \"train\":\n","                    data = np.concatenate((X, y.reshape(-1, 1)), axis = 1)\n","                    np.random.shuffle(data)\n","                    X, y = data[:,:-1], data[:,-1]\n","                    y = y.reshape(-1, )\n","                    \n","                # X (50000, 784), y (50000, )\n","                sample_size, _ = X.shape\n","                batches = int(np.ceil(sample_size / self.batch_size))\n","                \n","                for j in range(batches):\n","                    b_start = j * self.batch_size\n","                    b_end = min(sample_size, (j+1) * self.batch_size)\n","                    batch_X = X[b_start:b_end, :]\n","                    batch_y = y[b_start:b_end]\n","                    cache = self.forward(batch_X)\n","                    if mode == \"train\":\n","                        grads = self.backward(cache, batch_y)\n","                        self.update(grads, self.lr)\n","\n","                    ho = cache[\"ho\"]\n","                    #factor =  (b_end - b_start) / b_end\n","                    #loss += (self.loss(ho, batch_y) - loss) * factor\n","                    #acc += (self.accuracy(ho, batch_y) - acc) * factor\n","\n","                    loss += self.loss(ho, batch_y) * (b_end - b_start)\n","                    acc += self.accuracy(ho, batch_y) * (b_end - b_start)\n","                    if show_live_progress:\n","                        time_elapsed = time.time() - since_mode\n","                        print_progress(b_end/sample_size, \n","                                       time_elapsed, \n","                                       s_before, \n","                                       s_after)\n","                    # end batch loop\n","                \n","                loss /= sample_size\n","                acc /= sample_size\n","                \n","                losses[mode].append(loss)\n","                accuracies[mode].append(acc)\n","                time_elapsed = time.time() - since_mode\n","                \n","                s_after += \" {} loss: {:.4f}, acc: {:<7.2%}\".format(\n","                    mode, loss, acc)\n","                \n","                \n","                if acc > best_acc[mode]:\n","                    best_acc[mode] = acc\n","                    if mode == \"val\":\n","                        s_after += \" *\"\n","                        patience_used = 0\n","                else:\n","                    if mode == \"val\":\n","                        patience_used += 1\n","                        if early_stop and patience_used >= epoch_patience:\n","                            early_stopped = epoch - epoch_patience\n","\n","                if show_live_progress:\n","                    print_progress(1, time_elapsed, s_before, s_after)\n","                else:\n","                    #print(' {} loss: {:.4f} acc: {:<7.2%} '.format(\n","                    #    mode, loss, acc), end = \" \")   \n","                    pass\n","                # end mode loop\n","                \n","            time_elapsed = time.time() - since\n","            if show_live_progress:\n","                print_progress(1, time_elapsed, s_before, s_after)\n","            else:\n","                #print(' {:.0f}m{:.0f}s'.format(\n","                #    time_elapsed // 60, time_elapsed % 60), end = \"\") \n","                pass\n","            print()\n","\n","            if epoch % auto_save_interval == 0:\n","                self.save_model(model_path)\n","            \n","            if early_stopped > 0:\n","                print(\"Early stopped at epoch: {}\".format(early_stopped))\n","                break\n","            \n","            if best_acc['val'] >= 0.97:\n","                print(\"Accuracy on validate set arrives 0.97, stop training\")\n","                break\n","                \n","            # end epoch loop\n","                \n","        time_elapsed = time.time() - start\n","        print('Training complete in {:.0f}m {:.0f}s'.format(\n","            time_elapsed // 60, time_elapsed % 60))\n","        print('Best val Acc on val:   {:.2%}'.format(best_acc['val']))\n","                \n","        return (losses, accuracies, best_acc['val'])\n","    \n","    \n","    def test(self):\n","        # no need so far.\n","        pass\n","    \n","    def save_model(self, filename):\n","        with open(filename, 'wb') as output:  # Overwrites any existing file.\n","            pickle.dump(self, output, pickle.HIGHEST_PROTOCOL)\n","    \n","    def load_model(self, filename):\n","        with open(filename, 'rb') as file:\n","            self = pickle.load(file)\n","        \n","    def net_info(self):\n","        \"\"\"display basic net configurations, architecture, total parameters,\n","        etc.\n","        \"\"\"\n","        print(\"Activation function: {}\".format(self.activate_func))\n","        if self.dims is None:\n","            return\n","        print(\"{} hidden layers\".format(self.n_hidden))\n","        print(\"Mode of weghts initialization: {}\".format(self.weight_init_mode))\n","        if self.lr is not None:\n","            print(\"learning rate: {}\".format(self.lr))\n","        if self.activate_func is not None:\n","            print(\"activate function: {}\".format(self.activate_func))\n","        if self.batch_size is not None:\n","            print(\"mini_batch_size: {}\".format(self.batch_size))\n","        num_params = 0\n","        for i in range(len(self.dims)):\n","            print(\"Layer{}:{:>5} neurons.\".format(i, self.dims[i]), end = \" \")\n","            if i > 0:\n","                print(\"Weights:{}, Bias:{}\".format(\n","                    self.Ws[i].shape, self.bs[i].shape))\n","                num_params += self.dims[i] * (1 + self.dims[i-1])\n","            else:\n","                print(\"\")\n","        print(\"Total number of params:{}\".format(num_params))"],"execution_count":0,"outputs":[]},{"metadata":{"id":"IBnxgSDN5eZy","colab_type":"text"},"cell_type":"markdown","source":["### 2. Initialization"]},{"metadata":{"id":"I6oDZh5PWgGD","colab_type":"text"},"cell_type":"markdown","source":["#### 1). Model architecture and training the model using 3 different weights initialization methods.\n","\n","Turn to the output of this code cell for the architecture we use for this part of the question."]},{"metadata":{"id":"LAS5NIv9CLnk","colab_type":"code","outputId":"c46bfd58-ee62-4b2a-84d7-292fb714ccb3","executionInfo":{"status":"ok","timestamp":1550358466239,"user_tz":300,"elapsed":297,"user":{"displayName":"Qiang YE","photoUrl":"https://lh6.googleusercontent.com/-k-9mYNphfpc/AAAAAAAAAAI/AAAAAAAAARQ/zyID3sFntfM/s64/photo.jpg","userId":"15200210150486633975"}},"colab":{"base_uri":"https://localhost:8080/","height":233}},"cell_type":"code","source":["print(\"We will use the following values for hyper-parameters:\")\n","max_epochs = 10\n","mlp = NN(data_path = 'mnist.pkl.gz', batch_size = 32,\n","         activate_func = \"ReLU\", max_epochs = max_epochs)\n","mlp.initialize_weights(hidden_dims = (512, 256), init_mode = \"Zero\")\n","mlp.net_info()"],"execution_count":7,"outputs":[{"output_type":"stream","text":["We will use the following values for hyper-parameters:\n","Activation function: ReLU\n","2 hidden layers\n","Mode of weghts initialization: Zero\n","learning rate: 0.0001\n","activate function: ReLU\n","mini_batch_size: 32\n","Layer0:  784 neurons. \n","Layer1:  512 neurons. Weights:(512, 784), Bias:(512, 1)\n","Layer2:  256 neurons. Weights:(256, 512), Bias:(256, 1)\n","Layer3:   10 neurons. Weights:(10, 256), Bias:(10, 1)\n","Total number of params:535818\n"],"name":"stdout"}]},{"metadata":{"id":"DBHo5CV_bypO","colab_type":"text"},"cell_type":"markdown","source":["---\n","Run the following code cell will output the training process of the MLP using 3 different weight initialization methods:\n","(mannually modified for better layout)\n","```\n","weight init mode: Zero\n","hidden dims:(512, 256) lr:0.01, Relu, batch size:128,\n","max epochs:10, early stopping:True, patience:20\n","[Epoch  0/9 100.00% 0m11s] train loss: 2.3021, acc: 11.33%  val loss: 2.3017, acc: 11.35%  *\n","[Epoch  1/9 100.00% 0m11s] train loss: 2.3015, acc: 11.36%  val loss: 2.3013, acc: 11.35% \n","[Epoch  2/9 100.00% 0m11s] train loss: 2.3013, acc: 11.36%  val loss: 2.3011, acc: 11.35% \n","[Epoch  3/9 100.00% 0m11s] train loss: 2.3011, acc: 11.36%  val loss: 2.3011, acc: 11.35% \n","[Epoch  4/9 100.00% 0m11s] train loss: 2.3011, acc: 11.36%  val loss: 2.3010, acc: 11.35% \n","[Epoch  5/9 100.00% 0m11s] train loss: 2.3011, acc: 11.36%  val loss: 2.3010, acc: 11.35% \n","[Epoch  6/9 100.00% 0m11s] train loss: 2.3011, acc: 11.36%  val loss: 2.3010, acc: 11.35% \n","[Epoch  7/9 100.00% 0m11s] train loss: 2.3010, acc: 11.36%  val loss: 2.3010, acc: 11.35% \n","[Epoch  8/9 100.00% 0m11s] train loss: 2.3010, acc: 11.36%  val loss: 2.3010, acc: 11.35% \n","[Epoch  9/9 100.00% 0m11s] train loss: 2.3010, acc: 11.36%  val loss: 2.3010, acc: 11.35% \n","Training complete in 1m 51s\n","Best val Acc on val:   11.35%\n","\n","weight init mode: Normal\n","hidden dims:(512, 256) lr:0.01, Relu, batch size:128,\n","max epochs:10, early stopping:True, patience:20\n","[Epoch  0/9 100.00% 0m15s] train loss: 28.9835, acc: 82.85%  val loss: 15.3436, acc: 88.76%  *\n","[Epoch  1/9 100.00% 0m14s] train loss: 11.9952, acc: 89.82%  val loss: 11.4538, acc: 89.60%  *\n","[Epoch  2/9 100.00% 0m12s] train loss: 8.0931, acc: 91.25%  val loss: 9.7020, acc: 90.30%  *\n","[Epoch  3/9 100.00% 0m12s] train loss: 5.9010, acc: 92.47%  val loss: 8.7389, acc: 90.42%  *\n","[Epoch  4/9 100.00% 0m12s] train loss: 4.5341, acc: 93.27%  val loss: 7.8555, acc: 90.84%  *\n","[Epoch  5/9 100.00% 0m12s] train loss: 3.5709, acc: 93.94%  val loss: 7.3948, acc: 90.86%  *\n","[Epoch  6/9 100.00% 0m12s] train loss: 2.9484, acc: 94.53%  val loss: 7.0081, acc: 91.14%  *\n","[Epoch  7/9 100.00% 0m12s] train loss: 2.4454, acc: 94.99%  val loss: 6.8382, acc: 91.10% \n","[Epoch  8/9 100.00% 0m12s] train loss: 2.0985, acc: 95.36%  val loss: 6.7344, acc: 91.02% \n","[Epoch  9/9 100.00% 0m12s] train loss: 1.7856, acc: 95.65%  val loss: 6.2423, acc: 91.55%  *\n","Training complete in 2m 5s\n","Best val Acc on val:   91.55%\n","\n","weight init mode: Glorot\n","hidden dims:(512, 256) lr:0.01, Relu, batch size:128,\n","max epochs:10, early stopping:True, patience:20\n","[Epoch  0/9 100.00% 0m12s] train loss: 1.2099, acc: 73.47%  val loss: 0.5759, acc: 86.84%  *\n","[Epoch  1/9 100.00% 0m12s] train loss: 0.4859, acc: 87.96%  val loss: 0.3891, acc: 90.01%  *\n","[Epoch  2/9 100.00% 0m11s] train loss: 0.3781, acc: 89.85%  val loss: 0.3288, acc: 91.20%  *\n","[Epoch  3/9 100.00% 0m11s] train loss: 0.3321, acc: 90.86%  val loss: 0.2975, acc: 91.93%  *\n","[Epoch  4/9 100.00% 0m11s] train loss: 0.3039, acc: 91.57%  val loss: 0.2779, acc: 92.38%  *\n","[Epoch  5/9 100.00% 0m11s] train loss: 0.2836, acc: 92.03%  val loss: 0.2633, acc: 92.83%  *\n","[Epoch  6/9 100.00% 0m11s] train loss: 0.2673, acc: 92.51%  val loss: 0.2477, acc: 93.24%  *\n","[Epoch  7/9 100.00% 0m11s] train loss: 0.2535, acc: 92.85%  val loss: 0.2394, acc: 93.30%  *\n","[Epoch  8/9 100.00% 0m11s] train loss: 0.2415, acc: 93.15%  val loss: 0.2274, acc: 93.67%  *\n","[Epoch  9/9 100.00% 0m11s] train loss: 0.2311, acc: 93.45%  val loss: 0.2184, acc: 93.85%  *\n","Training complete in 1m 56s\n","Best val Acc on val:   93.85%\n","```"]},{"metadata":{"id":"luS45ilgalFI","colab_type":"code","colab":{}},"cell_type":"code","source":["init_modes = [\"Zero\", \"Normal\", \"Glorot\"]\n","mode_losses = []\n","for init_mode in init_modes:\n","    print(\"weight init mode: {}\".format(init_mode))\n","    mlp.initialize_weights(hidden_dims = (512, 256), init_mode = init_mode)\n","    losses, accuracies, best_acc = mlp.train(batch_size = 128, \n","                                             learning_rate = 1e-2,\n","                                             max_epochs = 10)\n","    mode_losses.append(losses)\n","    print(\"\")"],"execution_count":0,"outputs":[]},{"metadata":{"id":"I4XWLqvZdVB5","colab_type":"text"},"cell_type":"markdown","source":["----"]},{"metadata":{"id":"5kE4XPDwjhGk","colab_type":"text"},"cell_type":"markdown","source":["#### 2). Ploting and comments training loss on 10 epochs using three different weight initializaton methods"]},{"metadata":{"id":"Ao7NfcRtcKMl","colab_type":"text"},"cell_type":"markdown","source":["The curves are shown by executing codes in next cell. \n","\n","During the first 10 epochs with mini batch size = 32: \n","- Initialization with  **Zero** weights, training loss almost does not change, the model can learn nothing from the training dataset;\n","- Initialization with **Normal** weights, training loss decreases, while losses for first several epochs are extremely larger than losses using Zero and Glorot weight initialization methods. This could happen when **Cross Entropy** function is used to calculate loss, where the output(probability ) of true label can be much smaller than the average probability, 0.1 in this case. As training proceeds, the losses decrease quickly with a proper learning_rate supported.\n","- Initialization with **Glorot** weights, training loss decreases, with the initial probability output of true label very close to the average probability (0.1 in this cases)\n","\n","The **Glorot** weights initialization methods makes the weights more close to zero and uniformely distributed within a small range around,  which makes the output of the network more uniformely at the begining, while still keeping the ability to quickly learn from training dataset. Method **Zero** initializes all weights to zero such that all gradients are zero and the model cannot learn from training dataset. Method **Normal** assigns relatively large parameters resulting in a relatively large scale of initial loss and the decreasing of loss during epochs. "]},{"metadata":{"id":"NJdYI7V3bFt9","colab_type":"code","outputId":"ea59313b-c28b-4995-be4d-49fee558d55e","executionInfo":{"status":"ok","timestamp":1550258018295,"user_tz":300,"elapsed":1264,"user":{"displayName":"Qiang YE","photoUrl":"https://lh6.googleusercontent.com/-k-9mYNphfpc/AAAAAAAAAAI/AAAAAAAAARQ/zyID3sFntfM/s64/photo.jpg","userId":"15200210150486633975"}},"colab":{"base_uri":"https://localhost:8080/","height":512}},"cell_type":"code","source":["plt.figure(figsize = (10, 8))\n","epoch_data = np.arange(max_epochs)\n","for i in range(len(init_modes)):\n","    plt.plot(epoch_data, mode_losses[i][\"train\"], label = init_modes[i])\n","plt.xlabel('Epochs')\n","plt.ylabel('Losses')\n","plt.legend(loc='upper right')\n","plt.title(\"Loss on training dataset using different weight initialization style\")\n","plt.show()"],"execution_count":0,"outputs":[{"output_type":"display_data","data":{"image/png":"iVBORw0KGgoAAAANSUhEUgAAAlwAAAHvCAYAAACBqgH2AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAAAASUVORK5CYII=\n","text/plain":["<Figure size 720x576 with 1 Axes>"]},"metadata":{"tags":[]}}]},{"metadata":{"id":"ahLOGMEz5v2e","colab_type":"text"},"cell_type":"markdown","source":["### 3. Hyperparameter Search\n","\n","We use random search technique to select hyper-parameters settings.\n","\n","Below cell helps to search best hyper-parameters settings for our neural model training on MNIST dataset. \n","\n","Combinations of hyper-parameters we tried so far all achieved 97% accuracy on validation dataset.  We found that training progress on diferent values of `batch_size` and `learning_rate` differes a lot. Usually, certain number of batch_size will utilize the efficiency of matrix operation while decrease the speed of the convergence of our model.\n"," \n"," Below are the accuracies on validation dataset of different combinations of hyper-parameters we tried. The model will terminate training once the average accuracy on validation dataset achieves 97%.\n","\n","\\begin{array} \n","&\\text{hidden_dims}  & \\text{act_function} & \\text{learning_rate}   &      \\text{batch_size}     & \\text{best train accuracy}     & \\text{best_val_accuracy}  & \\text{epoch of best val acc}\\\\\n","\\hline\n","   (512, 256) & \\text{Sigmoid} & 0.1  & 1 &    98.04\\% & 97.53\\%     &        3 \\\\\n","   (512,256) & \\text{Relu} & 0.0001  & 1 &    97.82\\% & 97.00\\%     &  37        \\\\\n","   (512,256)* & \\text{Relu} & 0.01  & 32 &    97.82\\% & 97.07\\%     &      12    \\\\\n","   (512,512) & \\text{Sigmoid} & 0.1  & 32 &    97.85\\% & 97.14\\%     & 24         \\\\\n","   (512,512) & \\text{Sigmoid} & 0.1  & 1 &    98.27\\% & 97.56\\%     & 4        \\\\\n","   (512,512) & \\text{Relu} & 0.01  & 32 &    97.78\\% &  97.09\\%     & 12        \\\\\n","      \\hline\n"," \\end{array}\n"," \n","     * wil be used to compare with a CNN in problem 2\n"]},{"metadata":{"id":"Iwlp8hTdVDYb","colab_type":"code","outputId":"e877994a-aeed-4fd9-a24c-342e8b4017ab","executionInfo":{"status":"ok","timestamp":1550354261815,"user_tz":300,"elapsed":438363,"user":{"displayName":"Qiang YE","photoUrl":"https://lh6.googleusercontent.com/-k-9mYNphfpc/AAAAAAAAAAI/AAAAAAAAARQ/zyID3sFntfM/s64/photo.jpg","userId":"15200210150486633975"}},"colab":{"base_uri":"https://localhost:8080/","height":359}},"cell_type":"code","source":["hidden_dimses = [(512, 256)] #, (512, 512)]\n","learning_rates = [0.01] #, 1e-2, 1e-4]\n","activate_funcs = [\"Relu\"] #, \"Sigmoid\"]\n","mini_batch_sizes = [32] #1, 32, 256]\n","# if you once interrupted this code cell and want to continue try different\n","# hyper-parameter settings, you may insert the setting already trained in\n","# this list.\n","settings_trained = []\n","#settings_trained = [(0,0,0,0), (0,0,1,0), (1,0,1,1), (1,1,0,0), (1,1,1,0)]\n","\n","n_dims = len(hidden_dimses)\n","n_lrs =  len(learning_rates)\n","n_funcs = len(activate_funcs)\n","n_batch_size = len(mini_batch_sizes)\n","\n","# total times of searching is not bigger than number of all possibilities.\n","n_search = n_dims * n_lrs * n_funcs * n_batch_size \n","\n","mlp = NN(data_path = 'mnist.pkl.gz')\n","\n","for i in range(n_search):\n","    print(\"-\"* 30)\n","    dims_index, lr_index, func_index = 0, 0, 0\n","    while True:\n","        # generate a new hyper-parameter setting\n","        dims_index = np.random.randint(n_dims)\n","        lr_index = np.random.randint(n_lrs)\n","        func_index = np.random.randint(n_funcs)\n","        batch_size_index = np.random.randint(n_batch_size)\n","        \n","        setting = (dims_index, lr_index, func_index, batch_size_index)\n","        if not setting in settings_trained:\n","            # setting not trained\n","            break\n","    \n","    hidden_dims = hidden_dimses[dims_index]\n","    learning_rate = learning_rates[lr_index]\n","    activate_func = activate_funcs[func_index]\n","    batch_size = mini_batch_sizes[batch_size_index]\n","        \n","    mlp.initialize_weights(hidden_dims = hidden_dims, init_mode = \"Glorot\")\n","    losses, accuracies, best_acc = mlp.train(batch_size = batch_size, \n","                                             learning_rate = learning_rate,\n","                                             activate_func = activate_func,\n","                                             max_epochs = 50,\n","                                             epoch_patience = 5,\n","                                             show_live_progress = True)\n","    # add this setting to trained list.\n","    settings_trained.append(setting)\n","    # continue new hyper-parameter setting until all combinations are trained."],"execution_count":0,"outputs":[{"output_type":"stream","text":["------------------------------\n","hidden dims:(512, 256) lr:0.01, Relu, batch size:32,\n","max epochs:50, early stopping:True, patience:5\n","[Epoch  0/49 100.00% 0m33s] train loss: 0.6088, acc: 85.13%  val loss: 0.3034, acc: 91.72%  *\n","[Epoch  1/49 100.00% 0m34s] train loss: 0.2838, acc: 91.96%  val loss: 0.2451, acc: 93.11%  *\n","[Epoch  2/49 100.00% 0m33s] train loss: 0.2333, acc: 93.32%  val loss: 0.2052, acc: 94.25%  *\n","[Epoch  3/49 100.00% 0m34s] train loss: 0.1996, acc: 94.31%  val loss: 0.1844, acc: 94.72%  *\n","[Epoch  4/49 100.00% 0m34s] train loss: 0.1745, acc: 94.98%  val loss: 0.1650, acc: 95.18%  *\n","[Epoch  5/49 100.00% 0m34s] train loss: 0.1551, acc: 95.56%  val loss: 0.1533, acc: 95.56%  *\n","[Epoch  6/49 100.00% 0m34s] train loss: 0.1388, acc: 96.08%  val loss: 0.1359, acc: 95.85%  *\n","[Epoch  7/49 100.00% 0m34s] train loss: 0.1250, acc: 96.44%  val loss: 0.1325, acc: 96.03%  *\n","[Epoch  8/49 100.00% 0m33s] train loss: 0.1136, acc: 96.78%  val loss: 0.1182, acc: 96.53%  *\n","[Epoch  9/49 100.00% 0m33s] train loss: 0.1039, acc: 97.07%  val loss: 0.1113, acc: 96.64%  *\n","[Epoch 10/49 100.00% 0m34s] train loss: 0.0951, acc: 97.36%  val loss: 0.1066, acc: 96.62% \n","[Epoch 11/49 100.00% 0m33s] train loss: 0.0875, acc: 97.61%  val loss: 0.1047, acc: 96.85%  *\n","[Epoch 12/49 100.00% 0m33s] train loss: 0.0807, acc: 97.82%  val loss: 0.0991, acc: 97.07%  *\n","Accuracy on validate set arrives 0.97, stop training\n","Training complete in 7m 18s\n","Best val Acc on val:   97.07%\n"],"name":"stdout"}]},{"metadata":{"id":"QO9j9Ntw5mSY","colab_type":"text"},"cell_type":"markdown","source":["### 4. Validate Gradients using Finite Difference\n","\n","Two following functions are implemented to perform the gradient check using **one** training sample. \n","\n","In our implementation , gradient  check for **bias** is also supported. \n","\n","See comments of the codes for more details of the implementation.  "]},{"metadata":{"id":"1c8iEcWWLFNj","colab_type":"code","colab":{}},"cell_type":"code","source":["def grad_finite_diff(mlp, x, label, \n","                     layer_index = 1, \n","                     param_index = 0,\n","                     epsilon = 1e-5,\n","                     use_bias = False):\n","    \"\"\"compute gradient of a specified parameter using finite different method.\n","    on one training sample\n","    params\n","        x: one MNIST sample (1, 784)\n","        y: label, int\n","        layer_index: 1 means the first hidden layer\n","        param_index: 0 means the first wieght parameter in that layer\n","        use_bias: whether params_index is of bias matrix\n","        epsilon: a delta interval of parameter\n","    returns\n","        finite different gradient of the parameter, float\n","    \"\"\"\n","    # choose a param (weights or bias tensor of a certain layer\n","    param = mlp.Ws[layer_index] if not use_bias else mlp.bs[layer_index]\n","    # access of the index of the param\n","    h_layer, h_layer_prev = param.shape\n","    if not use_bias:\n","        i = param_index // h_layer_prev\n","        j = param_index % h_layer_prev\n","    else: # check gradient of a bias\n","        i = param_index\n","        j = 0\n","    # access of old value of a single scalar parameter \n","    old_value = param[i, j]    \n","    # prepare one training sample\n","    x = x.reshape(1, -1)\n","    label = np.array([label])\n","    \n","    # calculate loss of (old_value + epsilon)\n","    value_plus = old_value + epsilon\n","    param[i, j] = value_plus\n","    cache = mlp.forward(x)\n","    loss_plus = mlp.loss(cache[\"ho\"], label)\n","    #print(\"param_plus:{}, loss_plus: {}\".format(value_plus, loss_plus))\n","    \n","    # calculate loss of (old_value - epsilon)\n","    value_minor = old_value - epsilon\n","    param[i ,j] = value_minor\n","    cache = mlp.forward(x)\n","    loss_minor = mlp.loss(cache[\"ho\"], label)\n","    #print(\"param_minor:{}, loss_mino: {}\".format(value_minor, loss_minor))\n","    \n","    # compute gradient by finite difference method.\n","    grad_finite = (loss_plus - loss_minor)/(2.0 * epsilon)\n","    param[i, j] = old_value # restore old value.\n","\n","    #cache = mlp.forward(x)\n","    #grad_back = mlp.backward(cache, label)[0][layer_index][i, j]\n","    \n","    #print(grad_finite, grad_back)\n","    return grad_finite #, grad_back \n","\n","\n","def validate_gradient(mlp, num_params = 10, layer_index = 0, use_bias = False,\n","                     verbose = True):\n","    \"\"\"perform gradient check \n","    Params\n","        mlp: neural network object, NN\n","        num_params: number of parameters to be check, int\n","        layer_index: index where the parameters are located, int\n","        use_bias: if check bias gradient, bool\n","        verbose: display more information, bool\n","    Returns\n","        a cache dictionary with gradient check result and parameters\n","    \"\"\"\n","    X, labels = mlp.load_data(dataset_type = \"train\")\n","    sample_size = X.shape[0]\n","    # randomly choose a training sample\n","    sample_index = np.random.randint(0, sample_size)\n","    x = X[sample_index,:].reshape(1, -1)\n","    label = np.array([labels[sample_index]])\n","\n","    # ensure layer_index is valid\n","    if layer_index < 1 or layer_index > mlp.n_hidden + 1:\n","        layer_index = 1\n","    # locate the param tensor for gradient check\n","    param = mlp.Ws[layer_index] if not use_bias else mlp.bs[layer_index]\n","    param_type = 0 if not use_bias else 1\n","    param_names = [\"WEIGHTS\", \"BIAS\"]\n","    \n","    m = param.shape[0] * param.shape[1]\n","    # first p elements in parameter matrix\n","    p = m if num_params is None else min(num_params, m) \n","    \n","    # gradient using backward method.\n","    cache = mlp.forward(x)\n","    \n","    grads_back = mlp.backward(cache, label)[param_type][layer_index]\n","    # use first p gradient values\n","    grads_back_p = grads_back.reshape(-1, )[0:p] # to be compared \n","    \n","    if verbose:\n","        predict_label = np.argmax(cache[\"ho\"], axis = 1)\n","        loss = mlp.loss(cache[\"ho\"], label)\n","        print(\"validating gradient of {} of layer {}\".format(\n","            param_names[param_type], layer_index))\n","        print(\"num of total params:{}; p:{}\".format(m, p))    \n","        print(\"Sample: x.shape:{}, label:{}, predict_label:{}, loss:{}\"\n","              .format(x.shape, label, predict_label, loss))\n","        #print(\"first p={} backward grads:{}\".format(p, grads_back_p))\n","        print(\"{} non-zero of all {} backward grads\".format(\n","            np.count_nonzero(grads_back), m))\n","        print(\"{} non-zero of first {} backward grads\".format(\n","            np.count_nonzero(grads_back_p), p))\n","    #print(\"first p={} finite grads:\".format(p))\n","    max_diffs = []  # keep max difference for different epsilon/N\n","    # for different epsilon/N\n","    Ns = [k*np.power(10, i) for i in range(0, 6) for k in [1, 5] ]\n","    for N in Ns:\n","        epsilon = 1.0 / N\n","        grads_finite = np.zeros((p, ))\n","        for param_index in range(p):\n","            grads_finite[param_index] = grad_finite_diff(mlp, x, label, \n","                                                         layer_index, \n","                                                         param_index, \n","                                                         epsilon,\n","                                                         use_bias)\n","        #print(\"grads_finite:{}\".format(grads_finite))\n","        max_diff = np.max(np.abs(grads_finite - grads_back_p))\n","        max_diffs.append(max_diff)\n","        if verbose:\n","            print(\"N:{:<6}, epsilon:{:<6}, Grads All Close:{}, max_d_grads:{}\".\\\n","                  format(N, epsilon, np.allclose(grads_finite, grads_back_p), \n","                         max_diff))\n","    cache = {\"Ns\": Ns, \"max_diffs\": max_diffs, \"p\": p, \n","             \"layer_index\": layer_index, \"use_bias\": use_bias}            \n","    return cache"],"execution_count":0,"outputs":[]},{"metadata":{"id":"lLf2bCOv6DCh","colab_type":"text"},"cell_type":"markdown","source":["----\n","The architecture of the neural network used for gradient check is the output of the below code cell:\n","\n","```\n","Activation function: Relu\n","2 hidden layers\n","Mode of weghts initialization: Glorot\n","learning rate: 0.0001\n","activate function: Relu\n","mini_batch_size: 1\n","Layer0:  784 neurons. \n","Layer1:  512 neurons. Weights:(512, 784), Bias:(512, 1)\n","Layer2:  256 neurons. Weights:(256, 512), Bias:(256, 1)\n","Layer3:   10 neurons. Weights:(10, 256), Bias:(10, 1)\n","Total number of params:535818\n","```"]},{"metadata":{"id":"X0YBuoBU5bGh","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":215},"outputId":"405b40b4-7ce5-45ee-d94c-10782c19c0c7","executionInfo":{"status":"ok","timestamp":1550358510244,"user_tz":300,"elapsed":284,"user":{"displayName":"Qiang YE","photoUrl":"https://lh6.googleusercontent.com/-k-9mYNphfpc/AAAAAAAAAAI/AAAAAAAAARQ/zyID3sFntfM/s64/photo.jpg","userId":"15200210150486633975"}}},"cell_type":"code","source":["mlp = NN(data_path = 'mnist.pkl.gz', activate_func = \"Relu\")\n","mlp.initialize_weights(hidden_dims = (512, 256), \n","                       init_mode = 'Glorot', \n","                       random_seed = 10)\n","mlp.net_info()"],"execution_count":10,"outputs":[{"output_type":"stream","text":["Activation function: Relu\n","2 hidden layers\n","Mode of weghts initialization: Glorot\n","learning rate: 0.0001\n","activate function: Relu\n","mini_batch_size: 1\n","Layer0:  784 neurons. \n","Layer1:  512 neurons. Weights:(512, 784), Bias:(512, 1)\n","Layer2:  256 neurons. Weights:(256, 512), Bias:(256, 1)\n","Layer3:   10 neurons. Weights:(10, 256), Bias:(10, 1)\n","Total number of params:535818\n"],"name":"stdout"}]},{"metadata":{"id":"qwXAB5f16Ks6","colab_type":"text"},"cell_type":"markdown","source":["<hr>\n","The output of next code shows the max difference between two gradients from first p parameters of different N:\n","\n","```\n","validating gradient of WEIGHTS of layer 2\n","num of total params:131072; p:1000\n","Sample: x.shape:(1, 784), label:[8], predict_label:[4], loss:2.838183614164247\n","33750 non-zero of all 131072 backward grads\n","493 non-zero of first 1000 backward grads\n","N:1     , epsilon:1.0   , Grads All Close:False, max_d_grads:0.050374250712171637\n","N:5     , epsilon:0.2   , Grads All Close:False, max_d_grads:0.03171130070863891\n","N:10    , epsilon:0.1   , Grads All Close:False, max_d_grads:0.027701843271789026\n","N:50    , epsilon:0.02  , Grads All Close:True, max_d_grads:1.4545783480479635e-08\n","N:100   , epsilon:0.01  , Grads All Close:True, max_d_grads:3.636443382526444e-09\n","N:500   , epsilon:0.002 , Grads All Close:True, max_d_grads:1.4545810389510194e-10\n","N:1000  , epsilon:0.001 , Grads All Close:True, max_d_grads:3.6545225179374086e-11\n","N:5000  , epsilon:0.0002, Grads All Close:True, max_d_grads:1.7757427472897547e-12\n","N:10000 , epsilon:0.0001, Grads All Close:True, max_d_grads:3.0248442639546624e-12\n","N:50000 , epsilon:2e-05 , Grads All Close:True, max_d_grads:1.6712901895754584e-11\n","N:100000, epsilon:1e-05 , Grads All Close:True, max_d_grads:3.7237671279832796e-11\n","N:500000, epsilon:2e-06 , Grads All Close:True, max_d_grads:1.72866157152618e-10\n","```"]},{"metadata":{"id":"WGfNqZtvhLah","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":323},"outputId":"00ac2863-bfd9-48cf-af8e-b75c20a26827","executionInfo":{"status":"ok","timestamp":1550358523036,"user_tz":300,"elapsed":8930,"user":{"displayName":"Qiang YE","photoUrl":"https://lh6.googleusercontent.com/-k-9mYNphfpc/AAAAAAAAAAI/AAAAAAAAARQ/zyID3sFntfM/s64/photo.jpg","userId":"15200210150486633975"}}},"cell_type":"code","source":["cache = validate_gradient(mlp, num_params = 1000, layer_index = 2, \n","                  use_bias = False, \n","                  verbose = True)"],"execution_count":11,"outputs":[{"output_type":"stream","text":["validating gradient of WEIGHTS of layer 2\n","num of total params:131072; p:1000\n","Sample: x.shape:(1, 784), label:[2], predict_label:[4], loss:2.3610282108643967\n","36540 non-zero of all 131072 backward grads\n","510 non-zero of first 1000 backward grads\n","N:1     , epsilon:1.0   , Grads All Close:False, max_d_grads:0.07222879250391942\n","N:5     , epsilon:0.2   , Grads All Close:False, max_d_grads:0.04257867454717976\n","N:10    , epsilon:0.1   , Grads All Close:False, max_d_grads:0.0065789125485911926\n","N:50    , epsilon:0.02  , Grads All Close:True, max_d_grads:1.5106460166114388e-08\n","N:100   , epsilon:0.01  , Grads All Close:True, max_d_grads:3.7766230975844195e-09\n","N:500   , epsilon:0.002 , Grads All Close:True, max_d_grads:1.510787883685083e-10\n","N:1000  , epsilon:0.001 , Grads All Close:True, max_d_grads:3.783603985674233e-11\n","N:5000  , epsilon:0.0002, Grads All Close:True, max_d_grads:3.419126093362479e-12\n","N:10000 , epsilon:0.0001, Grads All Close:True, max_d_grads:3.973377182830973e-12\n","N:50000 , epsilon:2e-05 , Grads All Close:True, max_d_grads:2.089943669514316e-11\n","N:100000, epsilon:1e-05 , Grads All Close:True, max_d_grads:3.5369921401462445e-11\n","N:500000, epsilon:2e-06 , Grads All Close:True, max_d_grads:2.357235170946659e-10\n"],"name":"stdout"}]},{"metadata":{"id":"tLnXs-mP6ZeA","colab_type":"text"},"cell_type":"markdown","source":["<hr>\n","The curve of max difference of the gradients of N is shown as following. As N increases, the $\\epsilon$ decreases which leads to a smaller difference between gradients calculated by method `backward` and thoses by finite different method. For this case, when $N \\ge 10$, all the gradients monitored are very close, which indicates our `backward` methods may be correctly implemented. Furthermore, for each parameter in the neural network, its gradient can be automatically checked by the codes. We performed the check of most parameters, including first 1000 weights and all bias of all layers. The results indicate that our `backward` function is correctly implemented."]},{"metadata":{"id":"Z7a-rYLo8QgK","colab_type":"text"},"cell_type":"markdown","source":["The max difference increases slightly when $\\lg N > 4$ may be the result of the limitation of the representation accuracy from a division operation on extremely small float numbers."]},{"metadata":{"id":"kUDzAhUZ0VBW","colab_type":"code","outputId":"0213874e-71f1-43d1-ca5e-3e5c8177fd3b","executionInfo":{"status":"ok","timestamp":1550358532022,"user_tz":300,"elapsed":1488,"user":{"displayName":"Qiang YE","photoUrl":"https://lh6.googleusercontent.com/-k-9mYNphfpc/AAAAAAAAAAI/AAAAAAAAARQ/zyID3sFntfM/s64/photo.jpg","userId":"15200210150486633975"}},"colab":{"base_uri":"https://localhost:8080/","height":534}},"cell_type":"code","source":["plt.figure(figsize = (10, 8))\n","plt.plot(cache[\"Ns\"], cache[\"max_diffs\"])\n","plt.xlabel('Ns')\n","plt.ylabel('max_d_grads')\n","plt.xscale('log')\n","#plt.yscale('log')\n","param_type = \"Bias\" if cache[\"use_bias\"] else \"Weights\"\n","plt.title(\"Max difference of {} gradients of layer {} (p = {})\".format(\n","     param_type, cache[\"layer_index\"], cache[\"p\"]))\n","#plt.show()"],"execution_count":12,"outputs":[{"output_type":"execute_result","data":{"text/plain":["Text(0.5, 1.0, 'Max difference of Weights gradients of layer 2 (p = 1000)')"]},"metadata":{"tags":[]},"execution_count":12},{"output_type":"display_data","data":{"image/png":"iVBORw0KGgoAAAANSUhEUgAAAmUAAAHzCAYAAABhWGrkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\npB74ByrNNM13kpueAAAAAElFTkSuQmCC\n","text/plain":["<Figure size 720x576 with 1 Axes>"]},"metadata":{"tags":[]}}]},{"metadata":{"id":"RWOyHPy_YYtF","colab_type":"text"},"cell_type":"markdown","source":["\n","**(End of Report of Problem1)**\n","\n","---\n"]}]}